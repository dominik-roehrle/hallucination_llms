{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we create the test data for the application scenario. To avoid having trained samples in the test set we use the test set from the finetuned HoVer model and match it with the probes test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df_train_probe = pd.read_pickle('../Probes/processed_datasets_llama_layer-16/mini_fact_hover_train.pkl')\n",
    "df_test = pd.read_pickle('llama_finetuned_corrections/corrections_evidence_hover_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "docs_train_probe =  list(set(chain.from_iterable(df_train_probe['docs'])))\n",
    "df_test = df_test[~df_test['docs'].apply(lambda x: any(item in x for item in docs_train_probe))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to get the claims since they were ignored in previous research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_claim1 = pd.read_json(\"datasets_hover/hover_claim_evidence_generate1.json\")\n",
    "df_claim2 = pd.read_json(\"datasets_hover/hover_claim_evidence_generate2.json\")\n",
    "df_claim = pd.concat([df_claim1, df_claim2], ignore_index=True)\n",
    "df_claim['docs'] = df_claim['supporting_facts'].apply(lambda x: list({item[0] for item in x}))\n",
    "claim_list = []\n",
    "\n",
    "for index, row in df_test.iterrows():\n",
    "    docs = row['docs']\n",
    "    row = df_claim[df_claim['docs'].apply(lambda x: sorted(x) == sorted(docs))]\n",
    "    claim_list.append(row['claim'].values[0])\n",
    "    df_test.at[index, 'claim'] = row['claim'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.reset_index(drop=True, inplace=True)\n",
    "df_test = df_test[['claim', 'docs', 'ground_truth_source']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this give the final test set for the application scenario\n",
    "df_test.to_pickle('application/application_data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from itertools import chain\n",
    "\n",
    "df_train_finetuned = pd.read_pickle('train_datasets_combined/corrections_evidence_combined_train_balanced.pkl')\n",
    "docs_train_finetuned = list(set(chain.from_iterable(df_train_finetuned['docs'])))\n",
    "df_test = pd.read_pickle('llama_finetuned_corrections/corrections_evidence_hover_test.pkl')\n",
    "docs_test = list(set(chain.from_iterable(df_test['docs'])))\n",
    "\n",
    "gen_evidence_train = df_train_finetuned['gen_evidence'].tolist()\n",
    "gen_evidence_test = df_test['gen_evidence'].tolist()\n",
    "\n",
    "for gen_e in gen_evidence_train:\n",
    "    if gen_e in gen_evidence_test:\n",
    "        print(gen_e)\n",
    "\n",
    "for gen_e in gen_evidence_test:\n",
    "    if gen_e in gen_evidence_train:\n",
    "        print(gen_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in docs_train_finetuned:\n",
    "    if doc in docs_test:\n",
    "        print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in docs_test:\n",
    "    if doc in docs_train_finetuned:\n",
    "        print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in docs_train_probe:\n",
    "    if doc in docs_test:\n",
    "        print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in docs_test:\n",
    "    if doc in docs_train_probe:\n",
    "        print(doc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
