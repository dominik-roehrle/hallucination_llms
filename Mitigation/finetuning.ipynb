{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finetuning (inspired from https://www.kaggle.com/code/zivicmilos/llm-finetuning)\n",
    "\n",
    "import os \n",
    "import torch\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"7\"\n",
    "device = torch.device(f\"cuda:7\")\n",
    "\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model, AutoPeftModelForCausalLM\n",
    "from trl import SFTTrainer\n",
    "from random import randrange, sample, seed\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import EarlyStoppingCallback, TrainingArguments\n",
    "\n",
    "\n",
    "seed(42)\n",
    "\n",
    "\n",
    "from llm_interaction import LLMInteraction\n",
    "    \n",
    "\n",
    "def preprocess_data(train_file, dev_file, llm):\n",
    "    train_df = pd.read_pickle(train_file)\n",
    "    dev_df = pd.read_pickle(dev_file)\n",
    "    train_dataset = create_dataset(train_df, llm)\n",
    "    dev_dataset = create_dataset(dev_df, llm)\n",
    "    print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "    print(f\"Validation dataset size: {len(dev_dataset)}\")\n",
    "    return train_dataset, dev_dataset\n",
    "\n",
    "def create_dataset(df, llm):\n",
    "    messages = df.apply(\n",
    "        lambda x: llm.correct_mini_facts_prompt(\n",
    "            x['mini_facts_with_labels_false'], \n",
    "            x['ground_truth_source'], \n",
    "            x['correction_evidence'], \n",
    "            is_training=True\n",
    "        ), axis=1\n",
    "    ).tolist()\n",
    "    \n",
    "    dataset = Dataset.from_dict({\"chat\": messages})\n",
    "    dataset = dataset.map(\n",
    "        lambda x: {\"formatted_chat\": llm.llama_tokenizer.apply_chat_template(\n",
    "            x[\"chat\"], tokenize=False, add_generation_prompt=False)}\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# insert your model path\n",
    "model_path = \"\"\n",
    "\n",
    "\n",
    "llm = LLMInteraction(model_path, fine_tuned_version=False, few_shot=False, use_cache=False)\n",
    "\n",
    "llm.llama_model.config.pretraining_tp = 1\n",
    "llm.llama_tokenizer.pad_token = llm.llama_tokenizer.eos_token\n",
    "llm.llama_tokenizer.padding_side = \"right\"\n",
    "\n",
    "# your training files\n",
    "train_file = \"train_datasets_combined/corrections_evidence_combined_train_balanced.pkl\"\n",
    "dev_file = \"train_datasets_combined/corrections_evidence_combined_dev_balanced.pkl\"\n",
    "\n",
    "train_dataset, dev_dataset = preprocess_data(train_file, dev_file, llm)\n",
    "\n",
    "\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        r=64,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=[\n",
    "            \"q_proj\",\n",
    "            \"k_proj\",\n",
    "            \"v_proj\",\n",
    "            \"o_proj\",\n",
    "            \"gate_proj\", \n",
    "            \"up_proj\", \n",
    "            \"down_proj\",\n",
    "        ]\n",
    ")\n",
    "\n",
    "# Prepare model for training\n",
    "model = prepare_model_for_kbit_training(llm.llama_model)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"llama_finetuned_model\",\n",
    "    num_train_epochs=6,\n",
    "    per_device_train_batch_size=2, \n",
    "    gradient_accumulation_steps=4,\n",
    "    optim=\"adamw_8bit\",\n",
    "    logging_steps=1,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    eval_steps=1,\n",
    "    learning_rate=2e-4,\n",
    "    bf16=False,\n",
    "    fp16=True,\n",
    "    tf32=False,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_steps=5,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    disable_tqdm=False,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "\n",
    "early_stopping_callback = EarlyStoppingCallback(early_stopping_patience=2, early_stopping_threshold=0.05)\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=2048,\n",
    "    tokenizer=llm.llama_tokenizer,\n",
    "    packing=False,\n",
    "    dataset_text_field = \"formatted_chat\",\n",
    "    args=args,\n",
    "    callbacks=[early_stopping_callback]\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable()\n",
    "trainer.train()\n",
    "trainer.save_model()\n",
    "metrics = trainer.evaluate()\n",
    "print(f\"Validation loss after last evaluation: {metrics['eval_loss']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
