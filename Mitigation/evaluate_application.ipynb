{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook to evaluate the application scenarios with the FActScore prompt (we use GPT-4o-mini, optionally the labels can be checked with BART-large-MNLI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "with open(\"../api.key\", \"r\") as file:\n",
    "    api_key = file.read().strip() \n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm(messages, response_format):\n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages,\n",
    "        temperature=0.0,\n",
    "        max_tokens=256,\n",
    "        top_p=1.0,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        response_format={\n",
    "            \"type\": response_format\n",
    "        }\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeling_instruction = \"\"\"Please label each statement (the statements are seperated by a bullet point '- ') for truthfulness (True or False) based on the given context. \n",
    "Return the output in the JSON format where each statement is a key and the value is either True or False. The final format should be e.g.: {{'Mount Everest is a mountain': True, 'Mount Everest is the second highest mountain': False, 'Mount Everest is in the Himalaya': True}}\"\"\"\n",
    "\n",
    "def get_prompt_labeling(mini_facts, ground_truth_source):\n",
    "    messages = [{\"role\": \"system\", \n",
    "                \"content\" : [{\"type\": \"text\", \n",
    "                            \"text\":  f\"{labeling_instruction}\"}]},\n",
    "                {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": f\" Context: {ground_truth_source} \\nStatements: {mini_facts}\"}]}]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_facts_instruction = f\"\"\"Your task is to breakdown claims/sentences into independant statements. \n",
    "You must NEVER correct or comment the original claims/sentences even if something of the original claims/sentences is incorrect.\n",
    "Do NEVER generate statements that are not in the original claims/sentences. Every statement must start with an entity that specifies the topic (e.g. **The Fox Broadcasting Company** and not **The company**).\"\"\"\n",
    "        \n",
    "mini_facts_samples = [\"The Hunger Games is a 2012 American science fiction film directed by John Peter and based on the novel of the same name by Suzanne Collins. Matt Lucena is an American former professional tennis player.\",\n",
    "\"\"\"Owen Wilson starred in the film \"The Karate Kid\" (2010) alongside martial arts expert Tom Wu. Owen Wilson voiced Lightning McQueen in the \"Cars\" franchise, not \"The Royal Tenenbaums\" franchise.\"\"\",\n",
    "\"Feels So Good is a song by the American R&B group Tony! Toni! TonÃ. The song was written by the group's lead vocalist Raphael Saadiq and producer Tony! Toni! TonÃ's lead vocalist Dwayne Wimberly.\"]\n",
    "        \n",
    "        \n",
    "mini_facts_sample_outputs = [\"\"\"- **The Hunger Games** is a 2012 American science fiction film.\n",
    "- **The Hunger Games** was directed by John Peter.\n",
    "- **The Hunger Games** is based on a novel of the same name by Suzanne Collins.\n",
    "- **Matt Lucena** is an American former professional tennis player.\"\"\",\n",
    "\"\"\"- **Owen Wilson** starred in the film The Karate Kid (2010) alongside martial arts expert Tom Wu.\n",
    "- **Owen Wilson** voiced Lightning McQueen in the Cars franchise.\n",
    "- **Owen Wilson** did not voice Lightning McQueen in the The Royal Tenenbaums franchise.\"\"\",\n",
    "\"\"\"- **Feels So Good** is a song by the American R&B group Tony! Toni! TonÃ.\n",
    "- **Feels So Good** was written by the group's lead vocalist Raphael Saadiq and producer Tony! Toni! TonÃ's lead vocalist Dwayne Wimberly.\"\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_mini_facts(gen_evidence):\n",
    "    messages = [{\"role\": \"system\", \n",
    "            \"content\" : [{\"type\": \"text\", \n",
    "                        \"text\": f\"{mini_facts_instruction}\"}]},\n",
    "        {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": f\"{mini_facts_samples[0]}\"}]},\n",
    "        {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": f\"{mini_facts_sample_outputs[0]}\"}]},\n",
    "        {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": f\"{mini_facts_samples[1]}\"}]},\n",
    "        {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": f\"{mini_facts_sample_outputs[1]}\"}]},\n",
    "        {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": f\"{mini_facts_samples[2]}\"}]},\n",
    "        {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": f\"{mini_facts_sample_outputs[2]}\"}]},\n",
    "        {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": f\"{gen_evidence}\"}]}]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "\n",
    "# please insert BART model path\n",
    "bart_model_path = \"\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"Loading BART model...\")\n",
    "bart_model = AutoModelForSequenceClassification.from_pretrained(bart_model_path, local_files_only=True)\n",
    "bart_tokenizer = AutoTokenizer.from_pretrained(bart_model_path, local_files_only=True)\n",
    "bart_model.to(device)\n",
    "\n",
    "def split_source_to_fit_with_hypothesis(source, hypothesis, bart_tokenizer, max_length=1024):\n",
    "    \"\"\"Splits the source into chunks so that each chunk, when combined with the hypothesis, fits within the token limit.\"\"\"\n",
    "    original_max_length = bart_tokenizer.model_max_length\n",
    "    bart_tokenizer.model_max_length = int(1e12)  \n",
    "    hypothesis_tokens = bart_tokenizer.encode(hypothesis, add_special_tokens=False)\n",
    "    hypothesis_length = len(hypothesis_tokens)\n",
    "    num_special_tokens = bart_tokenizer.num_special_tokens_to_add(pair=True)\n",
    "    max_source_length = max_length - hypothesis_length - num_special_tokens\n",
    "    if max_source_length <= 0:\n",
    "        bart_tokenizer.model_max_length = original_max_length\n",
    "        raise ValueError(\"The hypothesis is too long to fit within the max_length limit.\")\n",
    "    source_tokens = bart_tokenizer.encode(source, add_special_tokens=False)\n",
    "    bart_tokenizer.model_max_length = original_max_length\n",
    "    token_chunks = [source_tokens[i:i+max_source_length] for i in range(0, len(source_tokens), max_source_length)]\n",
    "    text_chunks = [bart_tokenizer.decode(chunk, skip_special_tokens=True) for chunk in token_chunks]\n",
    "    return text_chunks\n",
    "    \n",
    "def call_bart_model(source, statement):\n",
    "    source_chunks = split_source_to_fit_with_hypothesis(source, statement, bart_tokenizer, max_length=1024)\n",
    "    entailment_probs = []\n",
    "    pred_labels = []\n",
    "    for idx, chunk in enumerate(source_chunks):\n",
    "        inputs = bart_tokenizer(\n",
    "            chunk,\n",
    "            statement,\n",
    "            return_tensors='pt',\n",
    "            truncation=True,\n",
    "            max_length=1024,\n",
    "            add_special_tokens=True\n",
    "        )\n",
    "        input_length = inputs['input_ids'].shape[1]\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = bart_model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            dominating_class = probs.argmax(dim=1).item()\n",
    "\n",
    "        class_names = [\"Contradiction\", \"Neutral\", \"Entailment\"]\n",
    "        prob_entailment = probs[:, 2].item()\n",
    "        entailment_probs.append(prob_entailment)\n",
    "        pred_labels.append(class_names[dominating_class])\n",
    "\n",
    "    filtered_labels = [label for label in pred_labels if label != \"Neutral\"]\n",
    "    if filtered_labels:\n",
    "        final_label = max(set(filtered_labels), key=pred_labels.count)\n",
    "    else:\n",
    "        final_label = max(set(pred_labels), key=pred_labels.count)\n",
    "    return final_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json \n",
    "\n",
    "# Load the application scenario\n",
    "df_evaluate = pd.read_pickle('application/whole_evidence.pkl')\n",
    "\n",
    "df_evaluate['gen_evidence'] = df_evaluate['gen_evidence'].str.replace(\" .###\", \".\").replace(\". .\", \".\")\n",
    "df_evaluate = df_evaluate[df_evaluate['gen_evidence'] != ' . .']\n",
    "df_evaluate = df_evaluate[df_evaluate['gen_evidence'] != ' .']\n",
    "\n",
    "df_true_bart_labels = pd.DataFrame()\n",
    "\n",
    "for index, row in df_evaluate.iterrows():\n",
    "    print(index)\n",
    "    gen_evidence = row['gen_evidence']\n",
    "    docs = row['docs']\n",
    "\n",
    "    messages = get_prompt_mini_facts(gen_evidence)\n",
    "    response = call_llm(messages, response_format=\"text\")\n",
    "    response = response.replace(\"**\", \"\")\n",
    "\n",
    "    ground_truth_source = row['Source']\n",
    "    messages = get_prompt_labeling(response, ground_truth_source)\n",
    "    response_format = \"json_object\"\n",
    "    response = call_llm(messages, response_format)\n",
    "    try:\n",
    "        response_json = json.loads(response)\n",
    "        response_data = [(key.strip(), int(value)) for key, value in response_json.items()]\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing JSON at index {index}: {e}. The response is skipped.\")\n",
    "        continue\n",
    "    \n",
    "    # this is optional\n",
    "    for mini_fact, label in response_data:\n",
    "        mini_fact = mini_fact.replace(\"- \", \"\")\n",
    "        pred_label = call_bart_model(ground_truth_source, mini_fact)\n",
    "        if pred_label == \"Entailment\" and label == 1:\n",
    "            bart_label = 1\n",
    "        elif pred_label == \"Contradiction\" and label == 0:\n",
    "            bart_label = 0\n",
    "        elif pred_label == \"Entailment\" and label == 0:\n",
    "            bart_label = 1\n",
    "        elif pred_label == \"Contradiction\" and label == 1:\n",
    "            bart_label = 0\n",
    "        df_true_bart_labels = pd.concat([df_true_bart_labels, pd.DataFrame({'gen_evidence': [gen_evidence], 'docs': [docs], 'Source': [ground_truth_source], 'Mini Fact': [mini_fact], 'Label': [label], 'Bart Label': [bart_label]})])\n",
    "\n",
    "# calculate accuracy \n",
    "df_true_bart_labels['Label'].sum() / len(df_true_bart_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
