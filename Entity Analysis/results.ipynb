{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally a PCA can be conducted here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_pickle(\"test/test_llm_generations.pkl\")\n",
    "\n",
    "labels = ['Very Low', 'Low', 'Medium', 'High', 'Very High']\n",
    "df['popularity_bin'] = pd.qcut(df['popularity'], q=5, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Determine the common sample size for each bin after class balancing\n",
    "min_bin_size = min(\n",
    "    min(len(group[group['label_mini_fact'] == 1]), len(group[group['label_mini_fact'] == 0]))\n",
    "    for _, group in df.groupby('popularity_bin')\n",
    ")\n",
    "\n",
    "# Step 2: Balance classes within each bin and resample to the common sample size\n",
    "balanced_dfs = []\n",
    "for bin_label, group in df.groupby('popularity_bin'):\n",
    "    # Separate the classes within the bin\n",
    "    positive_class = group[group['label_mini_fact'] == 1]\n",
    "    negative_class = group[group['label_mini_fact'] == 0]\n",
    "    \n",
    "    # Balance the classes by undersampling\n",
    "    positive_class_balanced = resample(positive_class, replace=False, n_samples=min_bin_size, random_state=42)\n",
    "    negative_class_balanced = resample(negative_class, replace=False, n_samples=min_bin_size, random_state=42)\n",
    "    print(f\"Balancing bin '{bin_label}' with {min_bin_size} samples in each class\")\n",
    "    \n",
    "    # Combine the balanced classes within the bin\n",
    "    balanced_group = pd.concat([positive_class_balanced, negative_class_balanced])\n",
    "    \n",
    "    # Resample the bin to the common bin size (2 * min_bin_size)\n",
    "    balanced_group = resample(balanced_group, replace=False, n_samples=2 * min_bin_size, random_state=42)\n",
    "    balanced_dfs.append(balanced_group)\n",
    "\n",
    "# Combine all balanced bins into a single DataFrame\n",
    "balanced_df = pd.concat(balanced_dfs).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import resample\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(16, 5)) \n",
    "popularity_bins = labels\n",
    "\n",
    "for i, bin_label in enumerate(popularity_bins):\n",
    "    filtered_df = balanced_df[balanced_df['popularity_bin'] == bin_label].copy()\n",
    "    X = list(filtered_df['embeddings-16_mini_fact'])\n",
    "    y = filtered_df['label_mini_fact'].values\n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    X_pca_positive = X_pca[y == 1]\n",
    "    X_pca_negative = X_pca[y == 0]\n",
    "    axes[i].scatter(X_pca_positive[:, 0], X_pca_positive[:, 1], c='yellow', alpha=0.7, label='Positive (Label 1)')\n",
    "    axes[i].scatter(X_pca_negative[:, 0], X_pca_negative[:, 1], c='purple', alpha=0.7, label='Negative (Label 0)')\n",
    "    axes[i].set_title(f\"{bin_label} Popularity\")\n",
    "    axes[i].set_xlabel(\"PCA Component 1\")\n",
    "    axes[i].set_ylabel(\"PCA Component 2\")\n",
    "    axes[i].set_aspect('equal')  \n",
    "    axes[i].legend()\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some Tests (can be skipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_pickle(\"train/train_injection_low_popularity_with_embeddings.pkl\")\n",
    "print(len(df_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_pickle(\"train/train_with_popularity_unbalanced.pkl\")\n",
    "print(len(df_train))\n",
    "\n",
    "df_train[\"closest_article\"] = df_train[\"closest_article\"].apply(lambda x: x[0] if isinstance(x, list) else x)\n",
    "\n",
    "df_test1 = pd.read_pickle(\"test/test_llm_generations.pkl\")\n",
    "df_test2 = pd.read_pickle(\"test/test_all_popularity_real_samples.pkl\")\n",
    "df_test = pd.concat([df_test1, df_test2], ignore_index=True)\n",
    "df_test[\"closest_article\"] = df_test[\"closest_article\"].apply(lambda x: x[0] if isinstance(x, list) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "#train_mini_facts_docs = list(set(chain.from_iterable(df_train['closest_article'])))\n",
    "train_mini_facts_docs = df_train['closest_article'].tolist()\n",
    "\n",
    "#test_mini_facts_docs = list(set(chain.from_iterable(df_test['docs'])))\n",
    "test_mini_facts_docs = df_test['closest_article'].tolist()\n",
    "\n",
    "for train_mini_facts_doc_t in train_mini_facts_docs:\n",
    "    if train_mini_facts_doc_t in test_mini_facts_docs:\n",
    "        print(train_mini_facts_doc_t)\n",
    "\n",
    "\n",
    "for test_mini_facts_doc_t in test_mini_facts_docs:\n",
    "    if test_mini_facts_doc_t in train_mini_facts_docs:\n",
    "        print(test_mini_facts_doc_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results: take the path from predictions to see the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_predictions1 = pd.read_pickle(\"predictions/prediction_with_train_popularity_unbalanced.pkl\")\n",
    "save_path1 = \"with_train_popularity_unbalanced\"\n",
    "df_predictions2 = pd.read_pickle(\"predictions/prediction_with_train_popularity_balanced.pkl\")\n",
    "save_path2 = \"with_train_popularity_balanced\"\n",
    "df_predictions3 = pd.read_pickle(\"predictions/prediction_with_real_samples_with_train_popularity_balanced.pkl\")\n",
    "save_path3 = \"with_real_samples_with_train_popularity_balanced\"\n",
    "df_predictions4 = pd.read_pickle(\"predictions/prediction_with_real_samples_with_train_popularity_balanced_train_injections.pkl\")\n",
    "save_path4 = \"with_real_samples_with_train_popularity_balanced_train_injections\"\n",
    "\n",
    "df = df_predictions4.copy()\n",
    "save_path = save_path4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bins are created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "labels = ['Very Low', 'Low', 'Medium', 'High', 'Very High']\n",
    "df['popularity_bin'] = pd.qcut(df['popularity'], q=5, labels=labels)\n",
    "\n",
    "bin_ranges = pd.qcut(df['popularity'], q=5).cat.categories\n",
    "\n",
    "bin_label_mapping = {label: bin_range for label, bin_range in zip(labels, bin_ranges)}\n",
    "\n",
    "print(\"Bin Ranges and Labels:\")\n",
    "for label, bin_range in bin_label_mapping.items():\n",
    "    print(f\"{label}: {bin_range}\")\n",
    "\n",
    "print(\"\\nNumber of Samples in Each Bin:\")\n",
    "sample_counts = df['popularity_bin'].value_counts(sort=False) \n",
    "for label, count in sample_counts.items():\n",
    "    print(f\"{label}: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "import pandas as pd\n",
    "\n",
    "min_bin_size = min(\n",
    "    min(len(group[group['label_mini_fact'] == 1]), len(group[group['label_mini_fact'] == 0]))\n",
    "    for _, group in df.groupby('popularity_bin')\n",
    ")\n",
    "\n",
    "balanced_dfs = []\n",
    "for bin_label, group in df.groupby('popularity_bin'):\n",
    "    positive_class = group[group['label_mini_fact'] == 1]\n",
    "    negative_class = group[group['label_mini_fact'] == 0]\n",
    "    \n",
    "    positive_class_balanced = resample(positive_class, replace=False, n_samples=min_bin_size, random_state=42)\n",
    "    negative_class_balanced = resample(negative_class, replace=False, n_samples=min_bin_size, random_state=42)\n",
    "    print(f\"Balancing bin '{bin_label}' with {min_bin_size} samples in each class\")\n",
    "    \n",
    "    balanced_group = pd.concat([positive_class_balanced, negative_class_balanced])\n",
    "    \n",
    "    balanced_group = resample(balanced_group, replace=False, n_samples=2 * min_bin_size, random_state=42)\n",
    "    balanced_dfs.append(balanced_group)\n",
    "\n",
    "balanced_df = pd.concat(balanced_dfs).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "print(\"Size of each bin:\")\n",
    "bin_sizes = balanced_df['popularity_bin'].value_counts().sort_index()\n",
    "print(bin_sizes)\n",
    "\n",
    "# Calculate F1 Scores for positive and negative classes for each bin\n",
    "f1_scores_positive = {}\n",
    "f1_scores_negative = {}\n",
    "auroc_scores = {}\n",
    "\n",
    "for bin_label, group in balanced_df.groupby('popularity_bin'):\n",
    "    predictions = (group['pred_prob'] >= 0.5).astype(int)\n",
    "    if group['label_mini_fact'].nunique() > 1:\n",
    "        f1_scores_positive[bin_label] = f1_score(group['label_mini_fact'], predictions)\n",
    "        f1_scores_negative[bin_label] = f1_score(1 - group['label_mini_fact'], 1 - predictions)\n",
    "        auroc_scores[bin_label] = roc_auc_score(group['label_mini_fact'], group['pred_prob'])\n",
    "    else:\n",
    "        f1_scores_positive[bin_label] = np.nan\n",
    "        f1_scores_negative[bin_label] = np.nan\n",
    "        auroc_scores[bin_label] = np.nan\n",
    "\n",
    "f1_df_positive = pd.DataFrame(list(f1_scores_positive.items()), columns=['popularity_bin', 'f1_score_positive'])\n",
    "f1_df_negative = pd.DataFrame(list(f1_scores_negative.items()), columns=['popularity_bin', 'f1_score_negative'])\n",
    "auroc_df = pd.DataFrame(list(auroc_scores.items()), columns=['popularity_bin', 'auroc'])\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(21*0.7, 6*0.7))\n",
    "\n",
    "\n",
    "axes[0].bar(f1_df_positive['popularity_bin'], f1_df_positive['f1_score_positive'], color='dodgerblue')\n",
    "axes[0].set_title('F1 Score (Positive Class)')\n",
    "axes[0].set_xlabel('Popularity Bin')\n",
    "axes[0].set_ylim(0.3, 1)  \n",
    "axes[0].set_ylabel('F1 Score')\n",
    "\n",
    "\n",
    "axes[1].bar(f1_df_negative['popularity_bin'], f1_df_negative['f1_score_negative'], color='tomato')\n",
    "axes[1].set_title('F1 Score (Negative Class)')\n",
    "axes[1].set_xlabel('Popularity Bin')\n",
    "axes[1].set_ylim(0.3, 1)  \n",
    "axes[1].set_ylabel('F1 Score')\n",
    "\n",
    "\n",
    "axes[2].bar(auroc_df['popularity_bin'], auroc_df['auroc'], color='grey')\n",
    "axes[2].set_title('AUROC for Each Popularity Bin')\n",
    "axes[2].set_xlabel('Popularity Bin')\n",
    "axes[2].set_ylim(0.3, 1)  \n",
    "axes[2].set_ylabel('AUROC')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "overall_auroc = roc_auc_score(balanced_df['label_mini_fact'], balanced_df['pred_prob'])\n",
    "print(f\"Overall AUROC: {overall_auroc:.4f}\")\n",
    "fig.savefig(f\"f1_{save_path}.png\", dpi=300, bbox_inches='tight')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prob Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 5), sharey=True) \n",
    "\n",
    "for i, bin_label in enumerate(labels):\n",
    "\n",
    "    bin_samples = balanced_df[balanced_df['popularity_bin'] == bin_label]\n",
    "    \n",
    "    positive_samples = bin_samples[bin_samples['label_mini_fact'] == 1]\n",
    "    negative_samples = bin_samples[bin_samples['label_mini_fact'] == 0]\n",
    "    \n",
    "    sns.histplot(positive_samples['pred_prob'], bins=20, kde=True, color='blue', label='Positive Samples (1)', ax=axes[i])\n",
    "    sns.histplot(negative_samples['pred_prob'], bins=20, kde=True, color='red', label='Negative Samples (0)', ax=axes[i])\n",
    "\n",
    "    title_text = f\"{bin_label}\\nPopularity\" \n",
    "    axes[i].set_title(title_text, fontsize=14) \n",
    "    axes[i].set_xlabel('Prediction Probability', fontsize=16)  \n",
    "\n",
    "    if i == 0:\n",
    "        axes[i].set_ylabel('Frequency', fontsize=16)  \n",
    "    \n",
    "\n",
    "    axes[i].tick_params(axis='x', labelsize=12) \n",
    "    axes[i].tick_params(axis='y', labelsize=12)  \n",
    "    \n",
    "\n",
    "    if i == 0:\n",
    "        axes[i].legend(fontsize=14) \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(f\"{save_path}.png\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 4), sharey=True)\n",
    "\n",
    "for i, bin_label in enumerate(labels):\n",
    "    bin_samples = balanced_df[balanced_df['popularity_bin'] == bin_label]\n",
    "    \n",
    "    positive_samples = bin_samples[bin_samples['label_mini_fact'] == 1]\n",
    "    negative_samples = bin_samples[bin_samples['label_mini_fact'] == 0]\n",
    "    \n",
    "    sns.histplot(positive_samples['pred_prob'], bins=20, kde=True, color='blue', label='Positive Samples (1)', ax=axes[i])\n",
    "    sns.histplot(negative_samples['pred_prob'], bins=20, kde=True, color='red', label='Negative Samples (0)', ax=axes[i])\n",
    "    \n",
    "    if i == 0:\n",
    "        axes[i].set_title(f\"Pred Prob Distribution - {bin_label} Popularity\")\n",
    "    else:\n",
    "        axes[i].set_title(f\"{bin_label} Popularity\")\n",
    "    axes[i].set_xlabel('Prediction Probability')\n",
    "    if i == 0:\n",
    "        axes[i].set_ylabel('Frequency')\n",
    "    \n",
    "    if i == 0:\n",
    "        axes[i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(\"probs_train_popularity_balanced_train_injections.png\", dpi=300, bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
