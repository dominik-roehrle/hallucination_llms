{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, f1_score\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class ProbeNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(ProbeNN, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 256)\n",
    "        self.layer2 = nn.Linear(256, 128)\n",
    "        self.layer3 = nn.Linear(128, 64)\n",
    "        self.output = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = torch.relu(self.layer2(x))\n",
    "        x = torch.relu(self.layer3(x))\n",
    "        x = torch.sigmoid(self.output(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_unbalanced_testset(test_pred_probs, df_test, accuracy_threshold):\n",
    "    df_pred = df_test.copy()\n",
    "    df_pred['pred_prob'] = test_pred_probs\n",
    "    df_pred['binary_pred'] = df_pred['pred_prob'].apply(lambda x: 1 if x > 0.5 else 0)\n",
    "\n",
    "    # Initialize counters in a dictionary\n",
    "    collect_counts = {'collect1s': 0, 'collect0s': 0}\n",
    "\n",
    "    # Define the evaluation function for each group\n",
    "    def evaluate_group(group, counts):\n",
    "        if group['label_mini_fact'].sum() == len(group):  # All labels are 1\n",
    "            counts['collect1s'] += 1\n",
    "            if group['binary_pred'].sum() == len(group):  # All predictions must be 1\n",
    "                return 'correct'\n",
    "            else:\n",
    "                return 'incorrect'\n",
    "        else:  # At least one label is 0\n",
    "            counts['collect0s'] += 1\n",
    "            if (group['binary_pred'] == 0).any():  # At least one prediction must be 0\n",
    "                return 'correct'\n",
    "            else:\n",
    "                return 'incorrect'\n",
    "\n",
    "    # Apply the evaluation function to each group\n",
    "    grouped_predictions = df_pred.groupby('gen_sentence').apply(lambda grp: evaluate_group(grp, collect_counts)).reset_index(name='group_prediction')\n",
    "    \n",
    "    num_correct = (grouped_predictions['group_prediction'] == 'correct').sum()\n",
    "    accuracy = num_correct / len(grouped_predictions)\n",
    "\n",
    "    # Calculate the AUC-ROC score\n",
    "    df_grouped = df_pred.groupby('gen_sentence').agg(\n",
    "        true_group_label=('label_mini_fact', lambda x: 1 if x.sum() == len(x) else 0),\n",
    "        pred_group_prob=('pred_prob', 'min')\n",
    "    ).reset_index()\n",
    "    \n",
    "    auc_roc_score = roc_auc_score(df_grouped['true_group_label'], df_grouped['pred_group_prob'])\n",
    "    return accuracy, collect_counts['collect1s'], collect_counts['collect0s'], auc_roc_score\n",
    "\n",
    "\n",
    "\n",
    "def compute_roc_curve(test_labels, test_pred_prob):\n",
    "    fpr, tpr, _ = roc_curve(test_labels, test_pred_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    return roc_auc, fpr, tpr \n",
    "\n",
    "\n",
    "def get_results(df_test, model_path, layer, probe_method, accuracy_threshold):\n",
    "    test_embeddings = np.array(df_test[f'embeddings{layer}_{probe_method}'].tolist())\n",
    "    test_labels = df_test[f'label_{probe_method}']\n",
    "\n",
    "    model = ProbeNN(test_embeddings.shape[1]).to(device)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_pred_prob = model(torch.tensor(test_embeddings, dtype=torch.float32).to(device)).cpu().numpy()\n",
    "\n",
    "    test_pred = (test_pred_prob > accuracy_threshold).astype(int)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    test_accuracy = accuracy_score(test_labels, test_pred)\n",
    "    \n",
    "    # Calculate ROC-AUC\n",
    "    roc_auc = roc_auc_score(test_labels, test_pred_prob)\n",
    "    \n",
    "    # Calculate F1 score for positive (label 1) and negative (label 0) classes\n",
    "    f1_score_positive = f1_score(test_labels, test_pred, pos_label=1)\n",
    "    f1_score_negative = f1_score(test_labels, test_pred, pos_label=0)\n",
    "    \n",
    "    return test_pred_prob, test_accuracy, roc_auc, f1_score_positive, f1_score_negative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_fever_test = []\n",
    "results_hover_test = []\n",
    "\n",
    "train_datasets = [\"hover\"]\n",
    "test_datasets = [\"hover\"]\n",
    "\n",
    "layer = -16\n",
    "probe = \"with_train_popularity_balanced\"\n",
    "test_real_samples = False\n",
    "\n",
    "df_test_mini_fact1 = pd.read_pickle(f\"./test/test_llm_generations.pkl\")\n",
    "\n",
    "if test_real_samples:\n",
    "    # we only use the LLMs negative samples\n",
    "    df_test_mini_fact1 = df_test_mini_fact1[df_test_mini_fact1['label_mini_fact'] == 0]\n",
    "    # here are only positive real samples\n",
    "    df_test_mini_fact2 = pd.read_pickle(f\"./test/test_all_popularity_real_samples.pkl\")\n",
    "    df_test_mini_fact2 = df_test_mini_fact2.sample(frac=1).reset_index(drop=True)\n",
    "    df_test_mini_fact = pd.concat([df_test_mini_fact1, df_test_mini_fact2], axis=0)\n",
    "else:\n",
    "    df_test_mini_fact = df_test_mini_fact1\n",
    "\n",
    "model_path_mini_fact = f\"./probes/{probe}.pth\"\n",
    "\n",
    "\n",
    "test_pred_probs_mini_fact, test_accuracy_mini_fact, roc_auc_mini_fact, f1_score_positive, f1_score_negative = get_results(df_test_mini_fact, \n",
    "                                                                                        model_path_mini_fact, \n",
    "                                                                                         layer=layer, \n",
    "                                                                                         probe_method=\"mini_fact\", accuracy_threshold=0.5)\n",
    "\n",
    "df_test_mini_fact['pred_prob'] = test_pred_probs_mini_fact\n",
    "df_test_mini_fact['binary_pred'] = df_test_mini_fact['pred_prob'].apply(lambda x: 1 if x > 0.5 else 0)\n",
    "\n",
    "if test_real_samples:\n",
    "    df_test_mini_fact.to_pickle(f\"predictions/prediction_with_real_samples_{probe}.pkl\")\n",
    "else:\n",
    "    df_test_mini_fact.to_pickle(f\"predictions/prediction_{probe}.pkl\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
