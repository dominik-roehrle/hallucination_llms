{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, f1_score\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class ProbeNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(ProbeNN, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 256)\n",
    "        self.layer2 = nn.Linear(256, 128)\n",
    "        self.layer3 = nn.Linear(128, 64)\n",
    "        self.output = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = torch.relu(self.layer2(x))\n",
    "        x = torch.relu(self.layer3(x))\n",
    "        x = torch.sigmoid(self.output(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_unbalanced_testset(test_pred_probs, df_test, accuracy_threshold):\n",
    "    df_pred = df_test.copy()\n",
    "    df_pred['pred_prob'] = test_pred_probs\n",
    "    df_pred['binary_pred'] = df_pred['pred_prob'].apply(lambda x: 1 if x > 0.5 else 0)\n",
    "\n",
    "    # Initialize counters in a dictionary\n",
    "    collect_counts = {'collect1s': 0, 'collect0s': 0}\n",
    "\n",
    "    # Define the evaluation function for each group\n",
    "    def evaluate_group(group, counts):\n",
    "        if group['label_mini_fact'].sum() == len(group):  # All labels are 1\n",
    "            counts['collect1s'] += 1\n",
    "            if group['binary_pred'].sum() == len(group):  # All predictions must be 1\n",
    "                return 'correct'\n",
    "            else:\n",
    "                return 'incorrect'\n",
    "        else:  # At least one label is 0\n",
    "            counts['collect0s'] += 1\n",
    "            if (group['binary_pred'] == 0).any():  # At least one prediction must be 0\n",
    "                return 'correct'\n",
    "            else:\n",
    "                return 'incorrect'\n",
    "\n",
    "    # Apply the evaluation function to each group\n",
    "    grouped_predictions = df_pred.groupby('gen_sentence').apply(lambda grp: evaluate_group(grp, collect_counts)).reset_index(name='group_prediction')\n",
    "    \n",
    "    num_correct = (grouped_predictions['group_prediction'] == 'correct').sum()\n",
    "    accuracy = num_correct / len(grouped_predictions)\n",
    "\n",
    "    # Calculate the AUC-ROC score\n",
    "    df_grouped = df_pred.groupby('gen_sentence').agg(\n",
    "        true_group_label=('label_mini_fact', lambda x: 1 if x.sum() == len(x) else 0),\n",
    "        pred_group_prob=('pred_prob', 'min')\n",
    "    ).reset_index()\n",
    "    \n",
    "    auc_roc_score = roc_auc_score(df_grouped['true_group_label'], df_grouped['pred_group_prob'])\n",
    "    return accuracy, collect_counts['collect1s'], collect_counts['collect0s'], auc_roc_score\n",
    "\n",
    "\n",
    "\n",
    "def compute_roc_curve(test_labels, test_pred_prob):\n",
    "    fpr, tpr, _ = roc_curve(test_labels, test_pred_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    return roc_auc, fpr, tpr \n",
    "\n",
    "\n",
    "def get_results(df_test, model_path, layer, probe_method, accuracy_threshold):\n",
    "    test_embeddings = np.array(df_test[f'embeddings{layer}_{probe_method}'].tolist())\n",
    "    test_labels = df_test[f'label_{probe_method}']\n",
    "\n",
    "    model = ProbeNN(test_embeddings.shape[1]).to(device)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_pred_prob = model(torch.tensor(test_embeddings, dtype=torch.float32).to(device)).cpu().numpy()\n",
    "\n",
    "    test_pred = (test_pred_prob > accuracy_threshold).astype(int)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    test_accuracy = accuracy_score(test_labels, test_pred)\n",
    "    \n",
    "    # Calculate ROC-AUC\n",
    "    roc_auc = roc_auc_score(test_labels, test_pred_prob)\n",
    "    \n",
    "    # Calculate F1 score for positive (label 1) and negative (label 0) classes\n",
    "    f1_score_positive = f1_score(test_labels, test_pred, pos_label=1)\n",
    "    f1_score_negative = f1_score(test_labels, test_pred, pos_label=0)\n",
    "    \n",
    "    return test_pred_prob, test_accuracy, roc_auc, f1_score_positive, f1_score_negative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_fever_test = []\n",
    "results_hover_test = []\n",
    "\n",
    "train_datasets = [\"hover\"]\n",
    "test_datasets = [\"hover\"]\n",
    "\n",
    "layer = -16\n",
    "probe = \"with_train_popularity_balanced\"\n",
    "test_real_samples = False\n",
    "\n",
    "df_test_mini_fact1 = pd.read_pickle(f\"./test/test_llm_generations.pkl\")\n",
    "\n",
    "if test_real_samples:\n",
    "    df_test_mini_fact1 = df_test_mini_fact1[df_test_mini_fact1['label_mini_fact'] == 0]\n",
    "    #df_test_mini_fact2 = pd.read_pickle(f\"./train_folder/mini_fact_hover_dev_with_popularity_unbalanced.pkl\")\n",
    "    #df_test_mini_fact2 = df_test_mini_fact2[df_test_mini_fact2['label_mini_fact'] == 0]\n",
    "    df_test_mini_fact3 = pd.read_pickle(f\"./test/test_all_popularity_real_samples.pkl\")\n",
    "    df_test_mini_fact3 = df_test_mini_fact3.sample(frac=1).reset_index(drop=True)\n",
    "    df_test_mini_fact = pd.concat([df_test_mini_fact1, df_test_mini_fact3], axis=0)\n",
    "else:\n",
    "    df_test_mini_fact = df_test_mini_fact1\n",
    "\n",
    "model_path_mini_fact = f\"./probes/{probe}.pth\"\n",
    "\n",
    "\n",
    "test_pred_probs_mini_fact, test_accuracy_mini_fact, roc_auc_mini_fact, f1_score_positive, f1_score_negative = get_results(df_test_mini_fact, \n",
    "                                                                                        model_path_mini_fact, \n",
    "                                                                                         layer=layer, \n",
    "                                                                                         probe_method=\"mini_fact\", accuracy_threshold=0.5)\n",
    "\n",
    "df_test_mini_fact['pred_prob'] = test_pred_probs_mini_fact\n",
    "df_test_mini_fact['binary_pred'] = df_test_mini_fact['pred_prob'].apply(lambda x: 1 if x > 0.5 else 0)\n",
    "\n",
    "if test_real_samples:\n",
    "    df_test_mini_fact.to_pickle(f\"predictions/prediction_with_real_samples_{probe}.pkl\")\n",
    "else:\n",
    "    df_test_mini_fact.to_pickle(f\"predictions/prediction_{probe}.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_pickle(\"test/sentence_hover_test_unbalanced.pkl\")\n",
    "\n",
    "df2 = pd.read_pickle(\"test/test_with_popularity_llm_generations.pkl\")\n",
    "df2.drop_duplicates(subset=['gen_evidence'], inplace=True)\n",
    "\n",
    "df_merged = pd.merge(df, df2, on='gen_evidence', how='left')\n",
    "\n",
    "\n",
    "test_pred_probs_sentences, test_accuracy_sentences, roc_auc_sentences ,f1_score_positive, f1_score_negative= get_results(df_test_mini_fact, \n",
    "                                                                                            model_path_mini_fact, \n",
    "                                                                                            layer=layer, \n",
    "                                                                                            probe_method=\"sentence\", accuracy_threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path_mini_fact = f\"./probes/mini_fact_embeddings{layer}_hover_with_popularity_unbalanced.pth\"\n",
    "test_pred_probs_mini_fact, test_accuracy_mini_fact, roc_auc_mini_fact, f1_score_positive, f1_score_negative = get_results(df_test_mini_fact, \n",
    "                                                                                                model_path_mini_fact, \n",
    "                                                                                                layer=layer, \n",
    "                                                                                                probe_method=\"mini_fact\", accuracy_threshold=0.5)\n",
    "\n",
    "\n",
    "df_test_mini_fact['pred_prob'] = test_pred_probs_mini_fact\n",
    "df_test_mini_fact['binary_pred'] = df_test_mini_fact['pred_prob'].apply(lambda x: 1 if x > 0.5 else 0)\n",
    "df_test_mini_fact.to_pickle(f\"train_folder/prediction_with_probe_popularity_unbalanced.pkl\")\n",
    "print(f1_score_positive)\n",
    "print(f1_score_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f1_score_positive)\n",
    "print(f1_score_negative)\n",
    "\n",
    "\n",
    "\n",
    "#if not balanced:\n",
    "#    correct_ratio, collect1s, collect0s, auc_roc_score = evaluate_unbalanced_testset(test_pred_probs_mini_fact, df_test_mini_fact, accuracy_threshold=0.5)\n",
    "    \n",
    "if test_dataset == \"fever\":\n",
    "    print(f\"Test dataset: {test_dataset}\")\n",
    "    results_fever_test.append({\n",
    "        \"train_dataset\": train_dataset,\n",
    "        \"test_dataset\": test_dataset,\n",
    "        \"layer\": str(33 + layer) + \"th-layer\",\n",
    "        #\"accuracySentence\": test_accuracy_sentences if balanced else None,\n",
    "        #\"auc_sentences\": roc_auc_sentences,\n",
    "        \"accuracyMiniFacts\": test_accuracy_mini_fact if balanced else None,\n",
    "        \"auc_mini_facts\": roc_auc_mini_fact,\n",
    "        #\"auc_mini_facts_sentences_match\": auc_roc_score if not balanced else None\n",
    "    })\n",
    "elif test_dataset == \"hover\":\n",
    "    print(f\"Test dataset: {test_dataset}\")\n",
    "    results_hover_test.append({\n",
    "        \"train_dataset\": train_dataset,\n",
    "        \"test_dataset\": test_dataset,\n",
    "        \"layer\": str(33 + layer) + \"th-layer\",\n",
    "        #\"accuracySentence\": test_accuracy_sentences if balanced else None,\n",
    "        #\"auc_sentences\": roc_auc_sentences,\n",
    "        \"accuracyMiniFacts\": test_accuracy_mini_fact if balanced else None,\n",
    "        \"auc_mini_facts\": roc_auc_mini_fact,\n",
    "        #\"auc_mini_facts_sentences_match\": auc_roc_score if not balanced else None\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_order = [str(33 + layer) + \"th-layer\" for layer in layers]\n",
    "\n",
    "\n",
    "# Function to prepare data\n",
    "def prepare_data(df, value_vars, index_name):\n",
    "    df_melted = df.melt(\n",
    "        id_vars=['train_dataset', 'test_dataset', 'layer'],\n",
    "        value_vars=value_vars,\n",
    "        var_name=index_name,\n",
    "        value_name='Value'\n",
    "    )\n",
    "    df_pivot = df_melted.pivot_table(\n",
    "        index=[index_name, 'train_dataset', 'test_dataset'],\n",
    "        columns='layer',\n",
    "        values='Value'\n",
    "    )\n",
    "    df_pivot.columns.name = None  # Remove multi-index for columns\n",
    "    df_pivot.reset_index(inplace=True)\n",
    "    return df_pivot\n",
    "\n",
    "# Prepare data for 'fever' and 'hover'\n",
    "fever_df = pd.DataFrame(results_fever_test)\n",
    "hover_df = pd.DataFrame(results_hover_test)\n",
    "\n",
    "if balanced:\n",
    "    fever_accuracy_pivot = prepare_data(fever_df, ['accuracySentence', 'accuracyMiniFacts'], 'Accuracy')\n",
    "    hover_accuracy_pivot = prepare_data(hover_df, ['accuracySentence', 'accuracyMiniFacts'], 'Accuracy')\n",
    "else:\n",
    "    fever_auroc_pivot = prepare_data(fever_df, ['auc_sentences', 'auc_mini_facts', 'auc_mini_facts_sentences_match'], 'AUROC')\n",
    "    hover_auroc_pivot = prepare_data(hover_df, ['auc_sentences', 'auc_mini_facts', 'auc_mini_facts_sentences_match'], 'AUROC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fever_accuracy_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# General function to generate LaTeX table lines with bold max value per dataset\n",
    "def generate_table_lines(train_dataset_name, test_dataset_name, df_pivot, metrics, metric_names):\n",
    "    lines = []\n",
    "    \n",
    "    for idx, metric in enumerate(metrics):\n",
    "        metric_name = metric_names[idx]\n",
    "        if metric_name == 'Sentences' and not balanced:\n",
    "            lines.append('\\\\addlinespace[11pt]')\n",
    "        #train_dataset_label = f\"\\\\textbf{{{train_dataset_name}}}\" if idx == 0 else ''\n",
    "        #test_dataset_label = f\"\\\\textbf{{{test_dataset_name}}}\" if idx == 0 else ''\n",
    "        train_test_label = f\"\\\\textbf{{{train_dataset_name}}} / \\\\textbf{{{test_dataset_name}}}\" if idx == 0 else ''\n",
    "        row_values = []\n",
    "\n",
    "        max_value_row = max(df_pivot.loc[df_pivot['Accuracy'] == metric].values[0][3:])\n",
    "        \n",
    "        for layer in layers_order:\n",
    "            value = df_pivot.loc[df_pivot['Accuracy'] == metric, layer].item()\n",
    "            formatted_value = f\"{value:.3f}\"\n",
    "            if np.isclose(float(value), max_value_row):  # Highlight max value\n",
    "                formatted_value = f\"\\\\textbf{{{formatted_value}}}\"\n",
    "            row_values.append(formatted_value)\n",
    "        \n",
    "        row = [train_test_label, metric_name] + row_values\n",
    "        lines.append(' & '.join(row) + ' \\\\\\\\')\n",
    "        \n",
    "        if metric == metrics[-1]:\n",
    "            lines.append('\\\\addlinespace[5pt]')  # Add space after last metric\n",
    "    return lines\n",
    "\n",
    "# Function to create a complete LaTeX table\n",
    "def create_latex_table(df_pivot_fever, df_pivot_hover, metrics, metric_names, caption, label):\n",
    "    header = ['Train/Test Set', 'Probe'] + [str(layer) for layer in layers_order]\n",
    "    header_line = ' & '.join(header) + ' \\\\\\\\ \\\\hline'\n",
    "    \n",
    "    table_lines = [header_line]\n",
    "\n",
    "    #if cross_test:\n",
    "    #    test_dataset_name = \"HoVer\"\n",
    "    #    train_dataset_name = \"FEVER\"\n",
    "    #else:\n",
    "    #    test_dataset_name = \"FEVER\"\n",
    "    #    train_dataset_name = \"FEVER\"\n",
    "    train_dataset_name =  \"FEVER\" if df_pivot_fever['train_dataset'].tolist()[0]==\"fever\" else \"HoVer\"\n",
    "    test_dataset_name = \"FEVER\" if df_pivot_fever['test_dataset'].tolist()[0]==\"fever\" else \"HoVer\"\n",
    "    table_lines.extend(generate_table_lines(train_dataset_name, test_dataset_name, df_pivot_fever, metrics, metric_names))\n",
    "    #table_lines.append('\\\\addlinespace[5pt]')\n",
    "    table_lines.append('\\\\hline')\n",
    "    table_lines.append('\\\\addlinespace[5pt]')\n",
    "\n",
    "    #if cross_test:\n",
    "    #    test_dataset_name = \"FEVER\"\n",
    "    #    train_dataset_name = \"HoVer\"\n",
    "    #else:\n",
    "    #    test_dataset_name = \"HoVer\"\n",
    "    #    train_dataset_name = \"HoVer\"\n",
    "    train_dataset_name =  \"FEVER\" if df_pivot_hover['train_dataset'].tolist()[0]==\"fever\" else \"HoVer\"\n",
    "    test_dataset_name = \"FEVER\" if df_pivot_hover['test_dataset'].tolist()[0]==\"fever\" else \"HoVer\"\n",
    "\n",
    "    table_lines.extend(generate_table_lines(train_dataset_name, test_dataset_name, df_pivot_hover, metrics, metric_names))\n",
    "    table_lines.append('\\\\hline')\n",
    "    \n",
    "    # Assemble the LaTeX table\n",
    "    latex_table = [\n",
    "        '\\\\begin{table}[h]',\n",
    "        '\\\\centering',\n",
    "        '\\\\small',\n",
    "        f'\\\\label{{{label}}}',\n",
    "        '\\\\begin{tabular}{p{1.5cm} p{4.5cm} r r r r r }',\n",
    "        '\\\\hline'\n",
    "    ]\n",
    "    latex_table.extend(table_lines)\n",
    "    latex_table.append('\\\\end{tabular}')\n",
    "    latex_table.append(f'\\\\caption{{{caption}}}')\n",
    "    latex_table.append('\\\\end{table}')\n",
    "    \n",
    "    return '\\n'.join(latex_table)\n",
    "\n",
    "\n",
    "if balanced:\n",
    "    # Create and print the Accuracy table\n",
    "    metrics_accuracy = ['accuracyMiniFacts', 'accuracySentence']\n",
    "    metric_names_accuracy = ['Mini Facts', 'Sentence']\n",
    "    latex_table_accuracy = create_latex_table(\n",
    "        fever_accuracy_pivot, hover_accuracy_pivot,\n",
    "        metrics_accuracy, metric_names_accuracy,\n",
    "        '\\\\textbf{Accuracy} Across Different Layers for balanced FEVER and HoVer test datasets',\n",
    "        'tab:accuracy_probes'\n",
    "    )\n",
    "    print(latex_table_accuracy)\n",
    "\n",
    "else:\n",
    "    # Create and print the AUROC table\n",
    "    metrics_auc = ['auc_mini_facts', 'auc_mini_facts_sentences_match', 'auc_sentences']\n",
    "    metric_names_auc = ['Mini Facts -> Mini Facts Level', 'Mini Facts -> Sentence Level', 'Sentences']\n",
    "    latex_table_auc = create_latex_table(\n",
    "        fever_auroc_pivot, hover_auroc_pivot,\n",
    "        metrics_auc, metric_names_auc,\n",
    "        \"\"\"\\\\textbf{AUROC} scores across Different Layers for unbalanced FEVER and HoVer test datasets\"\"\" if train_datasets[0]==test_datasets[0] else \"\"\"\\\\textbf{AUROC} scores for Cross Evaluation for HoVer probes on a FEVER test set and FEVER probes on a HoVer test set.\"\"\",\n",
    "        'tab:auroc_probes' if not train_datasets[0]==test_datasets[0] else 'tab:auroc_probes_cross_test'\n",
    "    )\n",
    "    print(latex_table_auc)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_probs_mini_fact, test_accuracy_mini_fact, roc_auc_mini_fact = get_results(df_test_mini_fact, \n",
    "                                                                                    model_path_mini_fact, \n",
    "                                                                                    layer=layer, \n",
    "                                                                                    probe_method=\"mini_fact\", accuracy_threshold=0.5)\n",
    "\n",
    "df_test_mini_fact['pred_prob'] = test_pred_probs_mini_fact\n",
    "df_test_mini_fact['binary_pred'] = df_test_mini_fact['pred_prob'].apply(lambda x: 1 if x > 0.5 else 0)\n",
    "true_count = 0\n",
    "\n",
    "for name, group in df_test_mini_fact.groupby('gen_sentence'):\n",
    "    print(name)\n",
    "    print(group['label_mini_fact'].values)\n",
    "\n",
    "    #print(group['pred_prob'].values)\n",
    "    #print(group['binary_pred'].values)\n",
    "    \"\"\"\n",
    "    if 0 in group['label_mini_fact'].values:\n",
    "        if 0 in group['binary_pred'].values:\n",
    "            print(\"True\")\n",
    "            true_count += 1\n",
    "        else:\n",
    "            print(\"False\")\n",
    "    else:\n",
    "        if 0 in group['binary_pred'].values:\n",
    "            print(\"False\")\n",
    "        else:\n",
    "            print(\"True\")\n",
    "            true_count += 1\n",
    "    \"\"\"\n",
    "    gen_sentence = df_test_sentence[df_test_sentence['output_sentence'] == name]['output_sentence'].values[0]\n",
    "    label_sentence = df_test_sentence[df_test_sentence['output_sentence'] == name]['label_sentence'].values[0]\n",
    "    print(gen_sentence)\n",
    "    print(label_sentence)\n",
    "    print(\"###\")\n",
    "\n",
    "#true_count / len(df_test_mini_fact['gen_sentence'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "df = pd.read_csv(\"./datasets_with_bart_hover/sentences_hover.csv\")\n",
    "\n",
    "df['docs'] = df['docs'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "#del df['concat_probs']\n",
    "\n",
    "df.to_pickle(\"./datasets_with_bart_hover/sentence_hover.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(\"./datasets_with_bart_fever/sentence_fever.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.to_pickle(\"./datasets_with_bart_hover/mini_facts_hover.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_str(str_arr):\n",
    "    val_to_ret = (str_arr.replace(\"[array(\", \"\")\n",
    "                        .replace(\"dtype=float32)]\", \"\")\n",
    "                        .replace(\"\\n\",\"\")\n",
    "                        .replace(\" \",\"\")\n",
    "                        .replace(\"],\",\"]\")\n",
    "                        .replace(\"[\",\"\")\n",
    "                        .replace(\"]\",\"\"))\n",
    "    return val_to_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"mini_facts_correction/atomic_facts_hover_gen_evidence_without_embeddings.csv\")\n",
    "\n",
    "df['docs'] = df['docs'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "del df['docs_y']\n",
    "del df['concat_probs']\n",
    "\n",
    "df.to_pickle(\"mini_facts_correction/mini_facts_hover_gen_evidence.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['docs'] = df['docs'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "del df['embeddings_sentence']\n",
    "\n",
    "\n",
    "df.to_pickle(f\"processed_datasets_{dataset}/{probe_method}_{dataset}.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
