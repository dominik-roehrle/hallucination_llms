{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fever\n",
      "AUROC for minimum probability: 0.6492\n",
      "AUROC for token weighted score: 0.6076\n",
      "AUROC for entropy: 0.6384\n",
      "hover\n",
      "AUROC for minimum probability: 0.6893\n",
      "AUROC for token weighted score: 0.6493\n",
      "AUROC for entropy: 0.6735\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "datasets = [\"fever\", \"hover\"]\n",
    "model_name = \"llama\"\n",
    "\n",
    "df_metrics = pd.DataFrame()\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(dataset)\n",
    "    df_sentence_test = pd.read_pickle(f\"probs_test_{model_name}/probs_sentence_{dataset}_with_token_importance.pkl\")\n",
    "\n",
    "\n",
    "\n",
    "    def calc_min_prob(probs):\n",
    "        \"\"\"Calculate the minimum probability from concat_probs.\"\"\"\n",
    "        return min(probs)\n",
    "\n",
    "\n",
    "    def calc_token_importance(token_importance, probs, pe):\n",
    "        \"\"\"Calculate the weighted score using token importance and probabilities.\"\"\"\n",
    "        log_probs = [-math.log(prob) * prob for prob in probs]\n",
    "        weighted_score = (token_importance / token_importance.sum()) * torch.tensor(log_probs)\n",
    "        return weighted_score.sum().item()\n",
    "\n",
    "    def calc_entropy(pe):\n",
    "        \"\"\"Calculate the entropy from concat_probs.\"\"\"\n",
    "        return sum(pe) / len(pe)\n",
    "\n",
    "    # Helper function to process each row of the DataFrame\n",
    "    def process_row(row):\n",
    "        concat_probs = row['concat_probs_sentence']\n",
    "\n",
    "        # llama\n",
    "        if model_name == \"llama\":\n",
    "            probs = [probs[2] for probs in concat_probs]\n",
    "            probs = [item for sublist in probs for item in sublist]\n",
    "            pe = [probs[3] for probs in concat_probs]\n",
    "            pe = [item for sublist in pe for item in sublist]\n",
    "\n",
    "        # phi \n",
    "        elif model_name == \"phi\":\n",
    "            probs = [probs[1] for probs in concat_probs]\n",
    "            pe = [probs[2] for probs in concat_probs]\n",
    "\n",
    "        token_importance = torch.tensor(row['token_importance'])\n",
    "        label = row['label_sentence']\n",
    "\n",
    "        if not probs:\n",
    "            return None\n",
    "        else:\n",
    "            # Calculate necessary values using the defined functions\n",
    "            min_prob = calc_min_prob(probs)\n",
    "            entropy = calc_entropy(pe)\n",
    "            weighted_score = calc_token_importance(token_importance, probs, pe)\n",
    "\n",
    "        \n",
    "        return pd.Series({\n",
    "            \"label_sentence\": label,\n",
    "            \"min_prob\": min_prob,\n",
    "            \"token_weighted_score\": weighted_score,\n",
    "            \"entropy\": entropy\n",
    "        })\n",
    "\n",
    "    # Main function to process the DataFrame using .apply()\n",
    "    def process_dataframe(df):\n",
    "        return df.apply(process_row, axis=1).dropna()\n",
    "\n",
    "    # Main execution\n",
    "    df_results = df_sentence_test.copy()\n",
    "\n",
    "    # Apply the processing function to each row of the DataFrame and store the results in a new DataFrame\n",
    "    df_results = process_dataframe(df_results)\n",
    "\n",
    "\n",
    "    def calculate_auroc(df, col_name, inverse=False):\n",
    "        if inverse:\n",
    "            labels = 1 - df[\"label_sentence\"]\n",
    "        else:\n",
    "            labels = df[\"label_sentence\"]\n",
    "        prob_scores = df[col_name]\n",
    "        auroc = roc_auc_score(labels, prob_scores)\n",
    "        return auroc\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # Calculate AUROC for each calculated metric\n",
    "    auroc_min_prob = calculate_auroc(df_results, \"min_prob\")\n",
    "    auroc_entropy = calculate_auroc(df_results, \"entropy\", inverse=True)\n",
    "    auroc_token_weighted_score = calculate_auroc(df_results, \"token_weighted_score\", inverse=True)\n",
    "\n",
    "\n",
    "    new_metrics_df = pd.DataFrame({\n",
    "        \"auroc\": [auroc_min_prob, auroc_entropy, auroc_token_weighted_score],\n",
    "        \"metric\": [\"min_prob\", \"entropy\", \"token_weighted_score\"],\n",
    "        \"dataset\": [dataset] * 3\n",
    "    })\n",
    "\n",
    "    df_metrics = pd.concat([df_metrics, new_metrics_df], ignore_index=True)\n",
    "\n",
    "    # Results\n",
    "    print(f\"AUROC for minimum probability: {auroc_min_prob:.4f}\")\n",
    "    print(f\"AUROC for token weighted score: {auroc_token_weighted_score:.4f}\")\n",
    "    print(f\"AUROC for entropy: {auroc_entropy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_order = ['avg_prob', 'min_prob', 'entropy', 'token_weighted_score']\n",
    "df_metrics['metric'] = pd.Categorical(df_metrics['metric'], categories=metric_order, ordered=True)\n",
    "\n",
    "df_pivot = df_metrics.pivot(index='metric', columns='dataset', values='auroc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_custom_latex_table(df_pivot, metrics, metric_names, caption, label):\n",
    "    # Start constructing the table with LaTeX syntax\n",
    "    table_lines = [\n",
    "        '\\\\begin{table}[h]',\n",
    "        '\\\\centering',\n",
    "        '\\\\small',\n",
    "        f'\\\\label{{{label}}}',\n",
    "        '\\\\begin{tabular}{l r r}',  # l: left-align for metric, r: right-align for values\n",
    "        '\\\\hline',\n",
    "        'Metric & Fever & Hover \\\\\\\\ \\\\hline'\n",
    "    ]\n",
    "    \n",
    "    # Loop through metrics and manually add rows\n",
    "    for metric, metric_name in zip(metrics, metric_names):\n",
    "        fever_value = df_pivot.loc[metric, 'fever']\n",
    "        hover_value = df_pivot.loc[metric, 'hover']\n",
    "        row = f'{metric_name} & {fever_value:.3f} & {hover_value:.3f} \\\\\\\\'\n",
    "        table_lines.append(row)\n",
    "    \n",
    "    # Finalize the table with LaTeX syntax\n",
    "    table_lines.append('\\\\hline')\n",
    "    table_lines.append('\\\\end{tabular}')\n",
    "    table_lines.append(f'\\\\caption{{{caption}}}')\n",
    "    table_lines.append('\\\\end{table}')\n",
    "    \n",
    "    return '\\n'.join(table_lines)\n",
    "\n",
    "# Example usage\n",
    "metrics = ['avg_prob', 'entropy', 'min_prob', 'token_weighted_score']\n",
    "metric_names = ['Average Probability', 'Minimum Probability', 'Entropy', 'TokenSAR']\n",
    "caption = 'Sentence Level Evaluation of Baselines on FEVER and HoVer Test Datasets'\n",
    "label = 'tab:metric_performance'\n",
    "\n",
    "# Assuming `df_pivot` is your pivoted DataFrame as created earlier\n",
    "latex_table_code = create_custom_latex_table(df_pivot, metrics, metric_names, caption, label)\n",
    "print(latex_table_code)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
