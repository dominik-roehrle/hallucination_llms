{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fever\n",
      "AUROC for average probability: 0.6245\n",
      "AUROC for maximum probability: 0.6492\n",
      "AUROC for token weighted score: 0.6518\n",
      "AUROC for entropy: 0.6921\n",
      "hover\n",
      "AUROC for average probability: 0.6630\n",
      "AUROC for maximum probability: 0.6893\n",
      "AUROC for token weighted score: 0.6993\n",
      "AUROC for entropy: 0.7155\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "datasets = [\"fever\", \"hover\"]\n",
    "model_name = \"llama\"\n",
    "\n",
    "df_metrics = pd.DataFrame()\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(dataset)\n",
    "    #df_probs_importance_sentences = pd.read_pickle(f\"datasets_{dataset}/probs_token_importance_sentence_with_bart_{dataset}.pkl\")\n",
    "    #df_sentence_test = pd.read_pickle(f\"probs_test_llama/df_new_{dataset}_probs_sentence_with_token_importance.pkl\")\n",
    "    df_sentence_test = pd.read_pickle(f\"probs_test_{model_name}/probs_sentence_{dataset}_with_token_importance.pkl\")\n",
    "\n",
    "    #df_merged = pd.merge(df_probs_importance_sentences, df_sentence_test, on=\"output_sentence\", how=\"inner\")\n",
    "    #df_merged.rename(columns={\"label_sentence_x\": \"label_sentence\"}, inplace=True)\n",
    "    #df_merged_clean = df_merged[['output_sentence', 'label_sentence', 'concat_probs_sentence', 'token_importance']]\n",
    "\n",
    "    # Define a helper function to flatten the nested lists of probabilities\n",
    "    def flatten_probs(concat_probs):\n",
    "        \"\"\"Flatten the nested lists of probabilities in concat_probs.\"\"\"\n",
    "        return [prob for _, prob_list in concat_probs for prob in prob_list]\n",
    "\n",
    "    # Define functions for specific calculations\n",
    "    def calc_min_prob(probs):\n",
    "        \"\"\"Calculate the minimum probability from concat_probs.\"\"\"\n",
    "        #flat_probs = flatten_probs(concat_probs)\n",
    "        log_probs = [-math.log(p) for p in probs]\n",
    "        # Return the maximum value from the log probabilities\n",
    "        return max(log_probs)\n",
    "\n",
    "    def calc_avg_prob(probs):\n",
    "        \"\"\"Calculate the average probability from concat_probs.\"\"\"\n",
    "        #flat_probs = flatten_probs(concat_probs)\n",
    "        log_probs = [-math.log(p) for p in probs]\n",
    "        # Compute the average of the log probabilities\n",
    "        avg_log_prob = sum(log_probs) / len(log_probs) if log_probs else 0.0\n",
    "        return avg_log_prob\n",
    "\n",
    "    def calc_token_importance(token_importance, probs, pe):\n",
    "        \"\"\"Calculate the weighted score using token importance and probabilities.\"\"\"\n",
    "        #flat_probs = flatten_probs(concat_probs)\n",
    "        log_probs = [-math.log(prob) * prob for prob in probs]\n",
    "        weighted_score = (token_importance / token_importance.sum()) * torch.tensor(pe)\n",
    "        return weighted_score.sum().item()\n",
    "\n",
    "    def calc_entropy(pe):\n",
    "        \"\"\"Calculate the entropy from concat_probs.\"\"\"\n",
    "        #flat_probs = flatten_probs(concat_probs)\n",
    "        #return -sum([prob * math.log(prob) for prob in probs])\n",
    "        return max(pe) \n",
    "\n",
    "    # Helper function to process each row of the DataFrame\n",
    "    def process_row(row):\n",
    "        concat_probs = row['concat_probs_sentence']\n",
    "       #print(concat_probs)\n",
    "\n",
    "        # llama\n",
    "        if model_name == \"llama\":\n",
    "            probs = [probs[2] for probs in concat_probs]\n",
    "            probs = [item for sublist in probs for item in sublist]\n",
    "            pe = [probs[3] for probs in concat_probs]\n",
    "            pe = [item for sublist in pe for item in sublist]\n",
    "\n",
    "        # phi \n",
    "        elif model_name == \"phi\":\n",
    "            probs = [probs[1] for probs in concat_probs]\n",
    "            pe = [probs[2] for probs in concat_probs]\n",
    "\n",
    "        #print(pe)\n",
    "        token_importance = torch.tensor(row['token_importance'])\n",
    "        label = row['label_sentence']\n",
    "\n",
    "        if not probs:\n",
    "            return None\n",
    "        else:\n",
    "            # Calculate necessary values using the defined functions\n",
    "            min_prob = calc_min_prob(probs)\n",
    "            avg_prob = calc_avg_prob(probs)\n",
    "            entropy = calc_entropy(pe)\n",
    "            weighted_score = calc_token_importance(token_importance, probs, pe)\n",
    "\n",
    "        \n",
    "\n",
    "        return pd.Series({\n",
    "            \"label_sentence\": label,\n",
    "            \"avg_prob\": avg_prob,\n",
    "            \"min_prob\": min_prob,\n",
    "            \"token_weighted_score\": weighted_score,\n",
    "            \"entropy\": entropy\n",
    "        })\n",
    "\n",
    "    # Main function to process the DataFrame using .apply()\n",
    "    def process_dataframe(df):\n",
    "        return df.apply(process_row, axis=1).dropna()\n",
    "\n",
    "    # Main execution\n",
    "    df_results = df_sentence_test.copy()\n",
    "\n",
    "    # Apply the processing function to each row of the DataFrame and store the results in a new DataFrame\n",
    "    df_results = process_dataframe(df_results)\n",
    "\n",
    "\n",
    "    def calculate_auroc(df, col_name, inverse=False):\n",
    "        if inverse:\n",
    "            labels = 1 - df[\"label_sentence\"]\n",
    "        else:\n",
    "            labels = df[\"label_sentence\"]\n",
    "        prob_scores = df[col_name]\n",
    "        auroc = roc_auc_score(labels, prob_scores)\n",
    "        return auroc\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # Calculate AUROC for each calculated metric\n",
    "    auroc_avg_prob = calculate_auroc(df_results, \"avg_prob\", inverse=True)\n",
    "    auroc_max_prob = calculate_auroc(df_results, \"min_prob\", inverse=True)\n",
    "    auroc_entropy = calculate_auroc(df_results, \"entropy\", inverse=True)\n",
    "    auroc_token_weighted_score = calculate_auroc(df_results, \"token_weighted_score\", inverse=True)\n",
    "\n",
    "\n",
    "    new_metrics_df = pd.DataFrame({\n",
    "        \"auroc\": [auroc_avg_prob, auroc_max_prob, auroc_entropy, auroc_token_weighted_score],\n",
    "        \"metric\": [\"avg_prob\", \"max_prob\", \"entropy\", \"token_weighted_score\"],\n",
    "        \"dataset\": [dataset] * 4\n",
    "    })\n",
    "\n",
    "    df_metrics = pd.concat([df_metrics, new_metrics_df], ignore_index=True)\n",
    "\n",
    "    # Results\n",
    "    print(f\"AUROC for average probability: {auroc_avg_prob:.4f}\")\n",
    "    print(f\"AUROC for maximum probability: {auroc_max_prob:.4f}\")\n",
    "\n",
    "\n",
    "    print(f\"AUROC for token weighted score: {auroc_token_weighted_score:.4f}\")\n",
    "    print(f\"AUROC for entropy: {auroc_entropy:.4f}\")\n",
    "\n",
    "\n",
    "    df_results['prob_label_avg'] = df_results.apply(lambda row: 1 if row['avg_prob'] >= 0.2 else 0, axis=1)\n",
    "    df_results['prob_label_min'] = df_results.apply(lambda row: 1 if row['min_prob'] >= 0.2 else 0, axis=1)\n",
    "\n",
    "    # Accuracy\n",
    "    #accuracy_avg = (df_results['prob_label_avg'] == df_results['label_sentence']).mean()\n",
    "    #accuracy_min = (df_results['prob_label_min'] == df_results['label_sentence']).mean()\n",
    "\n",
    "    # Results\n",
    "    #print(f\"Accuracy for average probability: {accuracy_avg:.4f}\") \n",
    "    #print(f\"Accuracy for minimum probability: {accuracy_min:.4f}\") \n",
    "    \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_order = ['avg_prob', 'min_prob', 'entropy', 'token_weighted_score']\n",
    "df_metrics['metric'] = pd.Categorical(df_metrics['metric'], categories=metric_order, ordered=True)\n",
    "\n",
    "df_pivot = df_metrics.pivot(index='metric', columns='dataset', values='auroc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>dataset</th>\n",
       "      <th>fever</th>\n",
       "      <th>hover</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metric</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>avg_prob</th>\n",
       "      <td>0.518684</td>\n",
       "      <td>0.627768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min_prob</th>\n",
       "      <td>0.647431</td>\n",
       "      <td>0.725818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entropy</th>\n",
       "      <td>0.591875</td>\n",
       "      <td>0.687290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token_weighted_score</th>\n",
       "      <td>0.518436</td>\n",
       "      <td>0.639406</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "dataset                  fever     hover\n",
       "metric                                  \n",
       "avg_prob              0.518684  0.627768\n",
       "min_prob              0.647431  0.725818\n",
       "entropy               0.591875  0.687290\n",
       "token_weighted_score  0.518436  0.639406"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}[h]\n",
      "\\centering\n",
      "\\small\n",
      "\\label{tab:metric_performance}\n",
      "\\begin{tabular}{l r r}\n",
      "\\hline\n",
      "Metric & Fever & Hover \\\\ \\hline\n",
      "Average Probability & 0.519 & 0.628 \\\\\n",
      "Minimum Probability & 0.592 & 0.687 \\\\\n",
      "Entropy & 0.647 & 0.726 \\\\\n",
      "TokenSAR & 0.518 & 0.639 \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n",
      "\\caption{Sentence Level Evaluation of Baselines on FEVER and HoVer Test Datasets}\n",
      "\\end{table}\n"
     ]
    }
   ],
   "source": [
    "def create_custom_latex_table(df_pivot, metrics, metric_names, caption, label):\n",
    "    # Start constructing the table with LaTeX syntax\n",
    "    table_lines = [\n",
    "        '\\\\begin{table}[h]',\n",
    "        '\\\\centering',\n",
    "        '\\\\small',\n",
    "        f'\\\\label{{{label}}}',\n",
    "        '\\\\begin{tabular}{l r r}',  # l: left-align for metric, r: right-align for values\n",
    "        '\\\\hline',\n",
    "        'Metric & Fever & Hover \\\\\\\\ \\\\hline'\n",
    "    ]\n",
    "    \n",
    "    # Loop through metrics and manually add rows\n",
    "    for metric, metric_name in zip(metrics, metric_names):\n",
    "        fever_value = df_pivot.loc[metric, 'fever']\n",
    "        hover_value = df_pivot.loc[metric, 'hover']\n",
    "        row = f'{metric_name} & {fever_value:.3f} & {hover_value:.3f} \\\\\\\\'\n",
    "        table_lines.append(row)\n",
    "    \n",
    "    # Finalize the table with LaTeX syntax\n",
    "    table_lines.append('\\\\hline')\n",
    "    table_lines.append('\\\\end{tabular}')\n",
    "    table_lines.append(f'\\\\caption{{{caption}}}')\n",
    "    table_lines.append('\\\\end{table}')\n",
    "    \n",
    "    return '\\n'.join(table_lines)\n",
    "\n",
    "# Example usage\n",
    "metrics = ['avg_prob', 'entropy', 'min_prob', 'token_weighted_score']\n",
    "metric_names = ['Average Probability', 'Minimum Probability', 'Entropy', 'TokenSAR']\n",
    "caption = 'Sentence Level Evaluation of Baselines on FEVER and HoVer Test Datasets'\n",
    "label = 'tab:metric_performance'\n",
    "\n",
    "# Assuming `df_pivot` is your pivoted DataFrame as created earlier\n",
    "latex_table_code = create_custom_latex_table(df_pivot, metrics, metric_names, caption, label)\n",
    "print(latex_table_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "dataset = \"hover\"\n",
    "\n",
    "df = pd.read_pickle(f\"probs_test_llama/probs_sentence_{dataset}_with_token_importance.pkl\")\n",
    "#df_sentence = pd.read_pickle(f\"processed_datasets_with_bart_{dataset}_layer-1/sentence_{dataset}_test_unbalanced.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'song', '\"After', 'the', 'News\"', 'is', 'by', 'the', 'band', 'The', 'Alan', 'Parsons', 'Project,', 'from', 'their', '1985', 'album', '\"Stereotomy\".']\n",
      "Thesong\"AftertheNews\"isbythebandTheAlanParsonsProject,fromtheir1985album\"Stereotomy\".\n",
      "The  \"After the News\" is by the band The Alan Parsons Project, from The Alan Parsons Project's 1985 album \"Stereotomy\".\n"
     ]
    }
   ],
   "source": [
    "for index, row in df.iterrows():\n",
    "    concat = row['concat_probs_sentence']\n",
    "    sentence = row['output_sentence']\n",
    "    #concat = [item for sublist in concat for item in sublist]\n",
    "    #print(concat)\n",
    "    tokens = [tokens[0] for tokens in concat]\n",
    "    words = [tokens[1] for tokens in concat]\n",
    "    #tokens = [item for sublist in tokens for item in sublist]\n",
    "    print(tokens)\n",
    "\n",
    "    print(\"\".join(tokens))\n",
    "    print(sentence.replace(tokens[1], \"\"))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_sentence))\n",
    "df_sentence.drop_duplicates(subset=['output_sentence'], inplace=True)\n",
    "print(len(df_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gen_e in df['output_sentence'].tolist():\n",
    "    if gen_e not in df_sentence['output_sentence'].tolist():\n",
    "        print(gen_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gen_e in df_sentence['output_sentence'].tolist():\n",
    "    if gen_e not in df['output_sentence'].tolist():\n",
    "        print(gen_e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
