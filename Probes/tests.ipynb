{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test to evaluate the quality of the mini-facts from the evidence with BART-large-MNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "bart_model_path = \"D:\\huggingface\\huggingface\\hub\\models--facebook--bart-large-mnli\\snapshots\\d7645e127eaf1aefc7862fd59a17a5aa8558b8ce\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Loading BART model...\")\n",
    "bart_model = AutoModelForSequenceClassification.from_pretrained(bart_model_path, local_files_only=True)\n",
    "bart_tokenizer = AutoTokenizer.from_pretrained(bart_model_path, local_files_only=True)\n",
    "bart_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_source_to_fit_with_hypothesis(source, hypothesis, bart_tokenizer, max_length=1024):\n",
    "    \"\"\"Splits the source into chunks so that each chunk, when combined with the hypothesis, fits within the token limit.\"\"\"\n",
    "    original_max_length = bart_tokenizer.model_max_length\n",
    "    bart_tokenizer.model_max_length = int(1e12)  \n",
    "    hypothesis_tokens = bart_tokenizer.encode(hypothesis, add_special_tokens=False)\n",
    "    hypothesis_length = len(hypothesis_tokens)\n",
    "    num_special_tokens = bart_tokenizer.num_special_tokens_to_add(pair=True)\n",
    "    max_source_length = max_length - hypothesis_length - num_special_tokens\n",
    "    if max_source_length <= 0:\n",
    "        bart_tokenizer.model_max_length = original_max_length\n",
    "        raise ValueError(\"The hypothesis is too long to fit within the max_length limit.\")\n",
    "    source_tokens = bart_tokenizer.encode(source, add_special_tokens=False)\n",
    "    bart_tokenizer.model_max_length = original_max_length\n",
    "    token_chunks = [source_tokens[i:i+max_source_length] for i in range(0, len(source_tokens), max_source_length)]\n",
    "    text_chunks = [bart_tokenizer.decode(chunk, skip_special_tokens=True) for chunk in token_chunks]\n",
    "    return text_chunks\n",
    "    \n",
    "def call_bart_model(source, statement):\n",
    "    source_chunks = split_source_to_fit_with_hypothesis(source, statement, bart_tokenizer, max_length=1024)\n",
    "    entailment_probs = []\n",
    "    pred_labels = []\n",
    "    for idx, chunk in enumerate(source_chunks):\n",
    "        inputs = bart_tokenizer(\n",
    "            chunk,\n",
    "            statement,\n",
    "            return_tensors='pt',\n",
    "            truncation=True,\n",
    "            max_length=1024,\n",
    "            add_special_tokens=True\n",
    "        )\n",
    "        input_length = inputs['input_ids'].shape[1]\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = bart_model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            dominating_class = probs.argmax(dim=1).item()\n",
    "\n",
    "        class_names = [\"Contradiction\", \"Neutral\", \"Entailment\"]\n",
    "        prob_entailment = probs[:, 2].item()\n",
    "        entailment_probs.append(prob_entailment)\n",
    "        pred_labels.append(class_names[dominating_class])\n",
    "\n",
    "    filtered_labels = [label for label in pred_labels if label != \"Neutral\"]\n",
    "    if filtered_labels:\n",
    "        final_label = max(set(filtered_labels), key=pred_labels.count)\n",
    "    else:\n",
    "        final_label = max(set(pred_labels), key=pred_labels.count)\n",
    "    return final_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "samples = 1000\n",
    "df = pd.read_pickle(\"datasets_fever/mini_fact_fever.pkl\").iloc[:samples]\n",
    "\n",
    "counter_entailment = 0\n",
    "grouped_df = df.groupby(\"gen_evidence\")\n",
    "\n",
    "for name, group in grouped_df:\n",
    "    mini_facts = group[\"output_mini_fact\"].tolist()\n",
    "    gen_evidence = group[\"gen_evidence\"].iloc[0]\n",
    "    for mini_fact in mini_facts:\n",
    "        label = call_bart_model(gen_evidence, mini_fact)\n",
    "        if label == \"Entailment\":\n",
    "            counter_entailment += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_entailment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests Probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "dataset = \"hover\"\n",
    "\n",
    "df1 = pd.read_pickle(f\"probs_test_llama/df_new_{dataset}_probs_sentence.pkl\")\n",
    "df2 = pd.read_pickle(f\"processed_datasets_with_bart_{dataset}_layer-1/sentence_{dataset}_test_unbalanced.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gen_evidence in df1[\"output_sentence\"].tolist():\n",
    "    if gen_evidence not in df2[\"output_sentence\"].tolist():\n",
    "        print(gen_evidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gen_evidence in df2[\"output_sentence\"].tolist():\n",
    "    if gen_evidence not in df1[\"output_sentence\"].tolist():\n",
    "        print(gen_evidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests Mini Facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "dataset = \"fever\"\n",
    "\n",
    "df_train = pd.read_pickle(f\"processed_datasets_with_bart_{dataset}_layer-8/mini_fact_{dataset}_train.pkl\")\n",
    "print(len(df_train))\n",
    "df_dev = pd.read_pickle(f\"processed_datasets_with_bart_{dataset}_layer-8/mini_fact_{dataset}_dev.pkl\")\n",
    "print(len(df_dev))\n",
    "df_test = pd.read_pickle(f\"processed_datasets_with_bart_{dataset}_layer-8/mini_fact_{dataset}_test_unbalanced.pkl\")\n",
    "print(len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_pickle(f\"processed_datasets_with_bart_{dataset}_layer-1/mini_fact_{dataset}_train.pkl\")\n",
    "df_test = pd.read_pickle(f\"processed_datasets_with_bart_{dataset}_layer-1/mini_fact_{dataset}_test_unbalanced.pkl\")\n",
    "\n",
    "\n",
    "all_docs1 = [item for sublist in df_train['docs'] for item in sublist]\n",
    "all_docs2 = [item for sublist in df_test['docs'] for item in sublist]\n",
    "\n",
    "mini_facts_train = df_train[\"output_mini_fact\"].tolist() \n",
    "mini_facts_test = df_test[\"output_mini_fact\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mini_fact in mini_facts_train:\n",
    "    if mini_fact in mini_facts_test:\n",
    "        print(mini_fact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mini_fact in mini_facts_test:\n",
    "    if mini_fact in mini_facts_train:\n",
    "        print(mini_fact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_docs1 = set(all_docs1)\n",
    "set_docs2 = set(all_docs2)\n",
    "\n",
    "# Check if there's any overlap\n",
    "common_docs = set_docs1.intersection(set_docs2)\n",
    "\n",
    "# Print the result\n",
    "if not common_docs:\n",
    "    print(\"No common documents found between df1 and df2.\")\n",
    "else:\n",
    "    print(\"Common documents found:\", common_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = \"fever\"\n",
    "\n",
    "df = pd.read_pickle(\"datasets_hover_llama/gen_evidence_hover.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = \"fever\"\n",
    "\n",
    "df_train = pd.read_pickle(f\"processed_datasets_llama_{dataset}_layer-1/sentence_{dataset}_train.pkl\")\n",
    "df_test = pd.read_pickle(f\"processed_datasets_llama_{dataset}_layer-1/sentence_{dataset}_test_unbalanced.pkl\")\n",
    "print(len(df_train))\n",
    "\n",
    "\n",
    "all_docs1 = [item for sublist in df_train['docs'] for item in sublist]\n",
    "all_docs2 = [item for sublist in df_test['docs'] for item in sublist]\n",
    "\n",
    "sentence_train = df_train[\"output_sentence\"].tolist() \n",
    "sentence_test = df_test[\"output_sentence\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in sentence_train:\n",
    "    if sentence in sentence_test:\n",
    "        print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in sentence_test:\n",
    "    if sentence in sentence_train:\n",
    "        print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_docs1 = set(all_docs1)\n",
    "set_docs2 = set(all_docs2)\n",
    "\n",
    "# Check if there's any overlap\n",
    "common_docs = set_docs1.intersection(set_docs2)\n",
    "\n",
    "# Print the result\n",
    "if not common_docs:\n",
    "    print(\"No common documents found between df1 and df2.\")\n",
    "else:\n",
    "    print(\"Common documents found:\", common_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mini Facts - Sentence Match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_sentence = pd.read_pickle(f\"processed_datasets_with_bart_{dataset}_layer-1/sentence_{dataset}_test_unbalanced.pkl\")\n",
    "df_test_mini_fact = pd.read_pickle(f\"processed_datasets_with_bart_{dataset}_layer-1/mini_fact_{dataset}_test_unbalanced.pkl\")\n",
    "\n",
    "\n",
    "def balance_dataframe(df, label_name):\n",
    "        df_label_1 = df[df[str(label_name)] == 1]\n",
    "        df_label_0 = df[df[str(label_name)] == 0]\n",
    "        min_class_count = min(len(df_label_1), len(df_label_0))\n",
    "        df_label_1_downsampled = df_label_1.sample(min_class_count, random_state=42)\n",
    "        df_label_0_downsampled = df_label_0.sample(min_class_count, random_state=42)\n",
    "        balanced_df = pd.concat([df_label_1_downsampled, df_label_0_downsampled])\n",
    "        return balanced_df.reset_index(drop=True)\n",
    "\n",
    "df_test_sentence = balance_dataframe(df_test_sentence, 'label_sentence')\n",
    "df_test_mini_fact = df_test_mini_fact[df_test_mini_fact['gen_sentence'].isin(df_test_sentence['output_sentence'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gen_s in df_test_mini_fact['gen_sentence'].tolist():\n",
    "    if gen_s not in df_test_sentence['output_sentence'].tolist():\n",
    "        print(gen_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gen_s in df_test_sentence['output_sentence'].tolist():\n",
    "    if gen_s not in df_test_mini_fact['gen_sentence'].tolist():\n",
    "        print(gen_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual Check that all mini facts remain in sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped = df_test_mini_fact.groupby(\"gen_sentence\")\n",
    "\n",
    "\n",
    "for name, group in df_grouped:\n",
    "    gen_sentence = group[\"gen_sentence\"].iloc[0]\n",
    "    print(gen_sentence)\n",
    "    mini_facts = group[\"output_mini_fact\"].tolist()\n",
    "    print(mini_facts)\n",
    "    print(\"###\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = \"hover\"\n",
    "\n",
    "df1 = pd.read_pickle(f\"probs_test_phi/probs_sentence_{dataset}.pkl\")\n",
    "df2 = pd.read_pickle(f\"processed_datasets_phi/sentence_{dataset}_layer-1_test_unbalanced.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gen_evidence in df1[\"output_sentence\"].tolist():\n",
    "    if gen_evidence not in df2[\"output_sentence\"].tolist():\n",
    "        print(gen_evidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gen_evidence in df2[\"output_sentence\"].tolist():\n",
    "    if gen_evidence not in df1[\"output_sentence\"].tolist():\n",
    "        print(gen_evidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_pickle(f\"processed_datasets_llama_{dataset}_layer-1/mini_fact_{dataset}_train.pkl\")\n",
    "df_test = pd.read_pickle(f\"processed_datasets_phi_llama/mini_fact_{dataset}_layer-1_test_unbalanced.pkl\")\n",
    "\n",
    "\n",
    "all_docs1 = [item for sublist in df_train['docs'] for item in sublist]\n",
    "all_docs2 = [item for sublist in df_test['docs'] for item in sublist]\n",
    "\n",
    "mini_facts_train = df_train[\"output_mini_fact\"].tolist() \n",
    "mini_facts_test = df_test[\"output_mini_fact\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mini_fact in mini_facts_train:\n",
    "    if mini_fact in mini_facts_test:\n",
    "        print(mini_fact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mini_fact in mini_facts_test:\n",
    "    if mini_fact in mini_facts_train:\n",
    "        print(mini_fact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_docs1 = set(all_docs1)\n",
    "set_docs2 = set(all_docs2)\n",
    "\n",
    "# Check if there's any overlap\n",
    "common_docs = set_docs1.intersection(set_docs2)\n",
    "\n",
    "# Print the result\n",
    "if not common_docs:\n",
    "    print(\"No common documents found between df1 and df2.\")\n",
    "else:\n",
    "    print(\"Common documents found:\", common_docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
