{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook to evaluate the probes with AUROC and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class ProbeNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(ProbeNN, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 256)\n",
    "        self.layer2 = nn.Linear(256, 128)\n",
    "        self.layer3 = nn.Linear(128, 64)\n",
    "        self.output = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = torch.relu(self.layer2(x))\n",
    "        x = torch.relu(self.layer3(x))\n",
    "        x = torch.sigmoid(self.output(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_unbalanced_testset(test_pred_probs, df_test, accuracy_threshold):\n",
    "    df_pred = df_test.copy()\n",
    "    df_pred['pred_prob'] = test_pred_probs\n",
    "    df_pred['binary_pred'] = df_pred['pred_prob'].apply(lambda x: 1 if x > 0.5 else 0)\n",
    "\n",
    "    # Initialize counters in a dictionary\n",
    "    collect_counts = {'collect1s': 0, 'collect0s': 0}\n",
    "\n",
    "    # Define the evaluation function for each group\n",
    "    def evaluate_group(group, counts):\n",
    "        if group['label_mini_fact'].sum() == len(group):  # All labels are 1\n",
    "            counts['collect1s'] += 1\n",
    "            if group['binary_pred'].sum() == len(group):  # All predictions must be 1\n",
    "                return 'correct'\n",
    "            else:\n",
    "                return 'incorrect'\n",
    "        else:  # At least one label is 0\n",
    "            counts['collect0s'] += 1\n",
    "            if (group['binary_pred'] == 0).any():  # At least one prediction must be 0\n",
    "                return 'correct'\n",
    "            else:\n",
    "                return 'incorrect'\n",
    "\n",
    "    grouped_predictions = df_pred.groupby('gen_sentence').apply(lambda grp: evaluate_group(grp, collect_counts)).reset_index(name='group_prediction')\n",
    "    \n",
    "    num_correct = (grouped_predictions['group_prediction'] == 'correct').sum()\n",
    "    accuracy = num_correct / len(grouped_predictions)\n",
    "\n",
    "    # Calculate AUROC\n",
    "    df_grouped = df_pred.groupby('gen_sentence').agg(\n",
    "        true_group_label=('label_mini_fact', lambda x: 1 if x.sum() == len(x) else 0),\n",
    "        pred_group_prob=('pred_prob', 'min')\n",
    "    ).reset_index()\n",
    "    \n",
    "    auc_roc_score = roc_auc_score(df_grouped['true_group_label'], df_grouped['pred_group_prob'])\n",
    "    return accuracy, collect_counts['collect1s'], collect_counts['collect0s'], auc_roc_score\n",
    "\n",
    "\n",
    "def compute_roc_curve(test_labels, test_pred_prob):\n",
    "    fpr, tpr, _ = roc_curve(test_labels, test_pred_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    return roc_auc, fpr, tpr \n",
    "\n",
    "\n",
    "def get_results(df_test, model_path, layer, probe_method, accuracy_threshold):\n",
    "    test_embeddings = np.array(df_test[f'embeddings{layer}_{probe_method}'].tolist())\n",
    "    test_labels = df_test[f'label_{probe_method}']\n",
    "\n",
    "    model = ProbeNN(test_embeddings.shape[1]).to(device)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_pred_prob = model(torch.tensor(test_embeddings, dtype=torch.float32).to(device)).cpu().numpy()\n",
    "\n",
    "    test_accuracy = accuracy_score(test_labels, test_pred_prob > accuracy_threshold)\n",
    "    roc_auc, fpr, tpr = compute_roc_curve(test_labels, test_pred_prob)\n",
    "    return test_pred_prob, test_accuracy, roc_auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: -1\n",
      "Test dataset: hover\n"
     ]
    }
   ],
   "source": [
    "results_fever_test = []\n",
    "results_hover_test = []\n",
    "\n",
    "\n",
    "train_datasets = [\"hover\"]\n",
    "test_datasets = [\"hover\"]\n",
    "\n",
    "model_name = \"llama\"\n",
    "layers = [-1]\n",
    "balanced = False\n",
    "\n",
    "#caption = \"\\\\textbf{AUROC} across different layers for FEVER and HoVer test datasets\"\n",
    "#caption = \"\\\\textbf{AUROC} scores across different layers for cross-testing: probes trained on HoVer are tested on FEVER, and probes trained on FEVER are tested on HoVer.\"\n",
    "#caption = \"\\\\textbf{AUROC} scores for probes trained on Llama3-8B embeddings and tested on Phi generations\"\n",
    "\n",
    "#label = \"tab:auroc_probes\"\n",
    "#label = \"tab:auroc_probes_cross_test\" \n",
    "#label = \"tab:auroc_probes_phi\"\n",
    "\n",
    "\n",
    "\n",
    "def balance_dataframe(df, label_name):\n",
    "    df_label_1 = df[df[str(label_name)] == 1]\n",
    "    df_label_0 = df[df[str(label_name)] == 0]\n",
    "    min_class_count = min(len(df_label_1), len(df_label_0))\n",
    "    df_label_1_downsampled = df_label_1.sample(min_class_count, random_state=42)\n",
    "    df_label_0_downsampled = df_label_0.sample(min_class_count, random_state=42)\n",
    "    balanced_df = pd.concat([df_label_1_downsampled, df_label_0_downsampled])\n",
    "    return balanced_df.reset_index(drop=True)\n",
    "\n",
    "for test_dataset, train_dataset in zip(test_datasets, train_datasets):\n",
    "    for layer in layers:\n",
    "        print(f\"Layer: {layer}\")\n",
    "        layer_name = (\n",
    "            \"First \\\\newline Hidden \\\\newline Layer\" if layer == 1 else\n",
    "            \"Hidden \\\\newline Layer: \\\\newline 9\" if layer == -24 else\n",
    "            \"Hidden \\\\newline Layer: \\\\newline 17\" if layer == -16 else\n",
    "            \"Hidden \\\\newline Layer: \\\\newline 25\" if layer == -8 else\n",
    "            \"Last \\\\newline Hidden \\\\newline Layer\" if layer == -1 else None\n",
    "        )\n",
    "        if model_name == \"llama\":\n",
    "            test_dataset_name = f\"processed_datasets_{model_name}_{test_dataset}_layer{layer}\"\n",
    "            if balanced:\n",
    "                df_test_mini_fact = pd.read_pickle(f\"./{test_dataset_name}/mini_fact_{test_dataset}_test_balanced.pkl\")\n",
    "                df_test_sentence = pd.read_pickle(f\"./{test_dataset_name}/sentence_{test_dataset}_test_balanced.pkl\")\n",
    "            else:\n",
    "                df_test_mini_fact = pd.read_pickle(f\"./{test_dataset_name}/mini_fact_{test_dataset}_test_unbalanced.pkl\")\n",
    "                df_test_sentence = pd.read_pickle(f\"./{test_dataset_name}/sentence_{test_dataset}_test_unbalanced.pkl\")\n",
    "                df_test_sentence = balance_dataframe(df_test_sentence, 'label_sentence')\n",
    "                df_test_mini_fact = df_test_mini_fact[df_test_mini_fact['gen_sentence'].isin(df_test_sentence['output_sentence'])]\n",
    "\n",
    "\n",
    "        if model_name == \"phi\":\n",
    "            test_dataset_name = f\"processed_datasets_{model_name}\"\n",
    "            df_test_mini_fact = pd.read_pickle(f\"./{test_dataset_name}/mini_fact_{test_dataset}_layer{layer}_test_unbalanced.pkl\")\n",
    "            df_test_sentence = pd.read_pickle(f\"./{test_dataset_name}/sentence_{test_dataset}_layer{layer}_test_unbalanced.pkl\")\n",
    "            df_test_sentence = balance_dataframe(df_test_sentence, 'label_sentence')\n",
    "            df_test_mini_fact = df_test_mini_fact[df_test_mini_fact['gen_sentence'].isin(df_test_sentence['output_sentence'])]\n",
    "            \n",
    "        model_path_sentence = f\"./probes/sentence_embeddings{layer}_{train_dataset}.pth\"\n",
    "        model_path_mini_fact = f\"./probes/mini_fact_embeddings{layer}_{train_dataset}.pth\"\n",
    "\n",
    "\n",
    "\n",
    "        test_pred_probs_sentences, test_accuracy_sentences, roc_auc_sentences = get_results(df_test_sentence, \n",
    "                                                                                            model_path_sentence, \n",
    "                                                                                            layer=layer, \n",
    "                                                                                            probe_method=\"sentence\", accuracy_threshold=0.5)\n",
    "        \n",
    "        test_pred_probs_mini_fact, test_accuracy_mini_fact, roc_auc_mini_fact = get_results(df_test_mini_fact, \n",
    "                                                                                                    model_path_mini_fact, \n",
    "                                                                                                    layer=layer, \n",
    "                                                                                                    probe_method=\"mini_fact\", accuracy_threshold=0.5)\n",
    "        \n",
    "        df_test_mini_fact['pred_prob'] = test_pred_probs_mini_fact\n",
    "        df_test_mini_fact['binary_pred'] = df_test_mini_fact['pred_prob'].apply(lambda x: 1 if x > 0.5 else 0)\n",
    "        df_test_mini_fact.to_pickle(f\"prediction.pkl\")\n",
    "\n",
    "        if not balanced:\n",
    "            correct_ratio, collect1s, collect0s, auc_roc_score_mini_facts = evaluate_unbalanced_testset(test_pred_probs_mini_fact, df_test_mini_fact, accuracy_threshold=0.5)\n",
    "\n",
    "        if test_dataset == \"fever\":\n",
    "            print(f\"Test dataset: {test_dataset}\")\n",
    "            results_fever_test.append({\n",
    "                \"train_dataset\": train_dataset,\n",
    "                \"test_dataset\": test_dataset,\n",
    "                \"layer\": layer_name,\n",
    "                \"accuracySentence\": test_accuracy_sentences if balanced else None,\n",
    "                \"auc_sentences\": roc_auc_sentences,\n",
    "                \"accuracyMiniFacts\": test_accuracy_mini_fact if balanced else None,\n",
    "                \"auc_mini_facts\": roc_auc_mini_fact,\n",
    "                \"auc_mini_facts_sentences_match\": auc_roc_score_mini_facts if not balanced else None\n",
    "            })\n",
    "        elif test_dataset == \"hover\":\n",
    "            print(f\"Test dataset: {test_dataset}\")\n",
    "            results_hover_test.append({\n",
    "                \"train_dataset\": train_dataset,\n",
    "                \"test_dataset\": test_dataset,\n",
    "                \"layer\": layer_name,\n",
    "                \"accuracySentence\": test_accuracy_sentences if balanced else None,\n",
    "                \"auc_sentences\": roc_auc_sentences,\n",
    "                \"accuracyMiniFacts\": test_accuracy_mini_fact if balanced else None,\n",
    "                \"auc_mini_facts\": roc_auc_mini_fact,\n",
    "                \"auc_mini_facts_sentences_match\": auc_roc_score_mini_facts if not balanced else None\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'train_dataset': 'hover',\n",
       "  'test_dataset': 'hover',\n",
       "  'layer': 'Last \\\\newline Hidden \\\\newline Layer',\n",
       "  'accuracySentence': None,\n",
       "  'auc_sentences': 0.7846093400217464,\n",
       "  'accuracyMiniFacts': None,\n",
       "  'auc_mini_facts': 0.8162134434837778,\n",
       "  'auc_mini_facts_sentences_match': 0.8105132352000586}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_hover_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_mini_fact = df_test_mini_fact.groupby('gen_sentence')\n",
    "\n",
    "for name, group in grouped_mini_fact:\n",
    "    print(name)\n",
    "    print(group['output_mini_fact'].tolist())\n",
    "    print(group['label_mini_fact'].tolist())\n",
    "    print(\"Probs: \", group['pred_prob'].tolist())\n",
    "    sentence_label = df_test_sentence.loc[df_test_sentence['output_sentence'] == name, 'label_sentence'].values[0]\n",
    "    print(sentence_label)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_probs_mini_fact, test_accuracy_mini_fact, roc_auc_mini_fact = get_results(df_test_mini_fact, \n",
    "                                                                                    model_path_mini_fact, \n",
    "                                                                                    layer=layer, \n",
    "                                                                                    probe_method=\"mini_fact\", accuracy_threshold=0.5)\n",
    "\n",
    "df_test_mini_fact['pred_prob'] = test_pred_probs_mini_fact\n",
    "df_test_mini_fact['binary_pred'] = df_test_mini_fact['pred_prob'].apply(lambda x: 1 if x > 0.5 else 0)\n",
    "true_count = 0\n",
    "\n",
    "for name, group in df_test_mini_fact.groupby('gen_sentence'):\n",
    "    print(name)\n",
    "    print(group['label_mini_fact'].values)\n",
    "\n",
    "    \n",
    "    if 0 in group['label_mini_fact'].values:\n",
    "        if 0 in group['binary_pred'].values:\n",
    "            print(\"True\")\n",
    "            true_count += 1\n",
    "        else:\n",
    "            print(\"False\")\n",
    "    else:\n",
    "        if 0 in group['binary_pred'].values:\n",
    "            print(\"False\")\n",
    "        else:\n",
    "            print(\"True\")\n",
    "            true_count += 1\n",
    "    \n",
    "    gen_sentence = df_test_sentence[df_test_sentence['output_sentence'] == name]['output_sentence'].values[0]\n",
    "    label_sentence = df_test_sentence[df_test_sentence['output_sentence'] == name]['label_sentence'].values[0]\n",
    "    print(gen_sentence)\n",
    "    print(label_sentence)\n",
    "    print(\"###\")\n",
    "\n",
    "true_count / len(df_test_mini_fact['gen_sentence'].unique())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
